{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 5,
>>>>>>> Stashed changes
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from utils.data_utils import AG_NEWS_DATASET\n",
    "from utils.constants import *\n",
    "from model.transformer import Transformer as Transformer_origin\n",
    "from utils.training import Learner\n",
<<<<<<< Updated upstream
=======
    "from utils.training_ema import Learner as ema_learner\n",
>>>>>>> Stashed changes
    "\n",
    "from quantization.fully_quantize import Model\n",
    "from quantization.pytorch_api import ModelQuant\n",
<<<<<<< Updated upstream
=======
    "from quantization.quantize import quantizer\n",
    "from quantization.fully_quantize import Model as fullyQuantModel\n",
    "from quantization.binarize import binarize\n",
    "\n",
    "from utils.train_utils import module_copy\n",
>>>>>>> Stashed changes
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "outputs": [],
=======
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original model received!\n"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "# load dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_dl, test_dl = AG_NEWS_DATASET(tokenizer).load_data()\n",
    "\n",
    "# create model\n",
<<<<<<< Updated upstream
    "model = Model(4,\n",
    "                tokenizer.vocab_size,\n",
    "                BASELINE_MODEL_NUMBER_OF_LAYERS,\n",
    "                BASELINE_MODEL_NUMBER_OF_HEADS,\n",
    "                BASELINE_MODEL_DIM)\n",
    "\n",
    "# model = quantizer(model, 8, True)\n",
    "\n",
    "# model = ModelQuant(4,\n",
    "#                 tokenizer.vocab_size,\n",
    "#                 BASELINE_MODEL_NUMBER_OF_LAYERS,\n",
    "#                 BASELINE_MODEL_NUMBER_OF_HEADS,\n",
    "#                 BASELINE_MODEL_DIMENSION)\n",
    "\n",
    "model.model_name = 'transformer'\n",
=======
    "original_model = Transformer(d_model=BASELINE_MODEL_DIM,\n",
    "                    d_ff=BASELINE_FFN_DIM,\n",
    "                    d_hidden=BASELINE_HIDDEN_DIM,\n",
    "                    h=BASELINE_MODEL_NUMBER_OF_HEADS,\n",
    "                    n_layers=BASELINE_MODEL_NUMBER_OF_LAYERS,\n",
    "                    n_class=4,\n",
    "                    vocab=tokenizer.vocab_size\n",
    "                   )\n",
    "\n",
    "\n",
    "model = Transformer(d_model=BASELINE_MODEL_DIM,\n",
    "                    d_ff=BASELINE_FFN_DIM,\n",
    "                    d_hidden=BASELINE_HIDDEN_DIM,\n",
    "                    h=BASELINE_MODEL_NUMBER_OF_HEADS,\n",
    "                    n_layers=BASELINE_MODEL_NUMBER_OF_LAYERS,\n",
    "                    n_class=4,\n",
    "                    vocab=tokenizer.vocab_size\n",
    "                   )\n",
    "\n",
    "\n",
    "binarize(model, 'ALL', binarize_layer='ir', skip_final=True, qk_only=False)\n",
>>>>>>> Stashed changes
    "\n",
    "# loss func\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# simple optimizer\n",
    "optim = Adam(model.parameters(), lr= 1e-4)\n",
    "scheduler = MultiStepLR(optim, milestones=[10,15], gamma=0.1)\n",
    "\n",
    "train_config ={'model': model,\n",
    "               'loss_fn': loss_fn,\n",
    "               'optim': optim,\n",
    "               'datasets': [train_dl, test_dl],\n",
    "               'epochs': 10,\n",
    "               'batch_size': BATCH_SIZE,\n",
    "               'scheduler': scheduler,\n",
<<<<<<< Updated upstream
    "               'exp_name': \"quant_all\",\n",
    "               }\n",
    "\n",
    "# training\n",
    "learner_ag_news = Learner(train_config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learner_ag_news.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
=======
    "               'exp_name': \"binarization_latent_ir\",\n",
    "               'epoch_start_quantization': 1\n",
    "               }\n",
    "\n",
    "# training\n",
    "learner_ag_news = ema_learner(train_config, ema=False, ir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current lr 1.00000e-04\n",
      "tensor([0.1000], device='cuda:0')\n",
      "Epoch: [0][0/3750]\tLoss 1.8368\tPrec@1 6.250\n",
      "Epoch: [0][100/3750]\tLoss 3.2459\tPrec@1 30.786\n",
      "Epoch: [0][200/3750]\tLoss 2.3394\tPrec@1 32.058\n",
      "Epoch: [0][300/3750]\tLoss 1.9500\tPrec@1 37.718\n",
      "Epoch: [0][400/3750]\tLoss 1.6496\tPrec@1 46.236\n",
      "Epoch: [0][500/3750]\tLoss 1.4297\tPrec@1 53.237\n",
      "Epoch: [0][600/3750]\tLoss 1.2779\tPrec@1 58.080\n",
      "Epoch: [0][700/3750]\tLoss 1.1550\tPrec@1 62.005\n",
      "Epoch: [0][800/3750]\tLoss 1.0561\tPrec@1 65.274\n",
      "Epoch: [0][900/3750]\tLoss 0.9825\tPrec@1 67.574\n",
      "Epoch: [0][1000/3750]\tLoss 0.9225\tPrec@1 69.521\n",
      "Epoch: [0][1100/3750]\tLoss 0.8680\tPrec@1 71.279\n",
      "Epoch: [0][1200/3750]\tLoss 0.8241\tPrec@1 72.632\n",
      "Epoch: [0][1300/3750]\tLoss 0.7856\tPrec@1 73.861\n",
      "Epoch: [0][1400/3750]\tLoss 0.7524\tPrec@1 74.949\n",
      "Epoch: [0][1500/3750]\tLoss 0.7253\tPrec@1 75.829\n",
      "Epoch: [0][1600/3750]\tLoss 0.7008\tPrec@1 76.601\n",
      "Epoch: [0][1700/3750]\tLoss 0.6774\tPrec@1 77.364\n",
      "Epoch: [0][1800/3750]\tLoss 0.6573\tPrec@1 78.033\n",
      "Epoch: [0][1900/3750]\tLoss 0.6426\tPrec@1 78.498\n",
      "Epoch: [0][2000/3750]\tLoss 0.6269\tPrec@1 78.993\n",
      "Epoch: [0][2100/3750]\tLoss 0.6104\tPrec@1 79.553\n",
      "Epoch: [0][2200/3750]\tLoss 0.5959\tPrec@1 80.011\n",
      "Epoch: [0][2300/3750]\tLoss 0.5826\tPrec@1 80.430\n",
      "Epoch: [0][2400/3750]\tLoss 0.5685\tPrec@1 80.923\n",
      "Epoch: [0][2500/3750]\tLoss 0.5534\tPrec@1 81.437\n",
      "Epoch: [0][2600/3750]\tLoss 0.5413\tPrec@1 81.845\n",
      "Epoch: [0][2700/3750]\tLoss 0.5313\tPrec@1 82.157\n",
      "Epoch: [0][2800/3750]\tLoss 0.5217\tPrec@1 82.471\n",
      "Epoch: [0][2900/3750]\tLoss 0.5127\tPrec@1 82.760\n",
      "Epoch: [0][3000/3750]\tLoss 0.5045\tPrec@1 83.032\n",
      "Epoch: [0][3100/3750]\tLoss 0.4972\tPrec@1 83.264\n",
      "Epoch: [0][3200/3750]\tLoss 0.4904\tPrec@1 83.508\n",
      "Epoch: [0][3300/3750]\tLoss 0.4831\tPrec@1 83.758\n",
      "Epoch: [0][3400/3750]\tLoss 0.4762\tPrec@1 83.977\n",
      "Epoch: [0][3500/3750]\tLoss 0.4698\tPrec@1 84.171\n",
      "Epoch: [0][3600/3750]\tLoss 0.4636\tPrec@1 84.380\n",
      "Epoch: [0][3700/3750]\tLoss 0.4579\tPrec@1 84.576\n",
      "Epoch[0] *Validation*: Prec@1 90.789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▎                                      | 1/10 [06:08<55:18, 368.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current lr 1.00000e-04\n",
      "tensor([0.2887], device='cuda:0')\n",
      "Switching to quantized model\n",
      "Epoch: [1][0/3750]\tLoss 11580.1279\tPrec@1 43.750\n",
      "Epoch: [1][100/3750]\tLoss 10947.5149\tPrec@1 77.630\n",
      "Epoch: [1][200/3750]\tLoss 7266.2076\tPrec@1 79.431\n",
      "Epoch: [1][300/3750]\tLoss 5752.5275\tPrec@1 80.357\n",
      "Epoch: [1][400/3750]\tLoss 4861.7291\tPrec@1 81.336\n",
      "Epoch: [1][500/3750]\tLoss 4316.7648\tPrec@1 81.868\n",
      "Epoch: [1][600/3750]\tLoss 3911.2161\tPrec@1 82.378\n",
      "Epoch: [1][700/3750]\tLoss 3600.7747\tPrec@1 82.846\n",
      "Epoch: [1][800/3750]\tLoss 3338.5557\tPrec@1 83.329\n",
      "Epoch: [1][900/3750]\tLoss 3139.9385\tPrec@1 83.470\n",
      "Epoch: [1][1000/3750]\tLoss 2976.5802\tPrec@1 83.523\n",
      "Epoch: [1][1100/3750]\tLoss 2820.7172\tPrec@1 83.603\n",
      "Epoch: [1][1200/3750]\tLoss 2700.1894\tPrec@1 83.574\n",
      "Epoch: [1][1300/3750]\tLoss 2583.1388\tPrec@1 83.647\n",
      "Epoch: [1][1400/3750]\tLoss 2473.0782\tPrec@1 83.750\n",
      "Epoch: [1][1500/3750]\tLoss 2383.7206\tPrec@1 83.709\n",
      "Epoch: [1][1600/3750]\tLoss 2303.4169\tPrec@1 83.661\n",
      "Epoch: [1][1700/3750]\tLoss 2226.6408\tPrec@1 83.719\n",
      "Epoch: [1][1800/3750]\tLoss 2154.7810\tPrec@1 83.761\n",
      "Epoch: [1][1900/3750]\tLoss 2097.4482\tPrec@1 83.594\n",
      "Epoch: [1][2000/3750]\tLoss 2033.4176\tPrec@1 83.632\n",
      "Epoch: [1][2100/3750]\tLoss 1971.5455\tPrec@1 83.737\n",
      "Epoch: [1][2200/3750]\tLoss 1912.0055\tPrec@1 83.853\n",
      "Epoch: [1][2300/3750]\tLoss 1858.3362\tPrec@1 83.951\n",
      "Epoch: [1][2400/3750]\tLoss 1805.1779\tPrec@1 84.069\n",
      "Epoch: [1][2500/3750]\tLoss 1751.9388\tPrec@1 84.255\n",
      "Epoch: [1][2600/3750]\tLoss 1702.3398\tPrec@1 84.427\n",
      "Epoch: [1][2700/3750]\tLoss 1658.9631\tPrec@1 84.507\n",
      "Epoch: [1][2800/3750]\tLoss 1616.2321\tPrec@1 84.593\n",
      "Epoch: [1][2900/3750]\tLoss 1575.5391\tPrec@1 84.706\n",
      "Epoch: [1][3000/3750]\tLoss 1539.3433\tPrec@1 84.754\n",
      "Epoch: [1][3100/3750]\tLoss 1504.3971\tPrec@1 84.806\n",
      "Epoch: [1][3200/3750]\tLoss 1472.8896\tPrec@1 84.818\n",
      "Epoch: [1][3300/3750]\tLoss 1441.6112\tPrec@1 84.888\n",
      "Epoch: [1][3400/3750]\tLoss 1412.8433\tPrec@1 84.905\n",
      "Epoch: [1][3500/3750]\tLoss 1383.1861\tPrec@1 84.984\n",
      "Epoch: [1][3600/3750]\tLoss 1354.4137\tPrec@1 85.055\n",
      "Epoch: [1][3700/3750]\tLoss 1326.5183\tPrec@1 85.127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████▌                                  | 2/10 [13:29<54:50, 411.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] *Validation*: Prec@1 86.461\n",
      "current lr 1.00000e-04\n",
      "tensor([0.8337], device='cuda:0')\n",
      "Epoch: [2][0/3750]\tLoss 145.0475\tPrec@1 93.750\n",
      "Epoch: [2][100/3750]\tLoss 366.1298\tPrec@1 76.856\n",
      "Epoch: [2][200/3750]\tLoss 315.9823\tPrec@1 77.690\n",
      "Epoch: [2][300/3750]\tLoss 279.6194\tPrec@1 78.571\n",
      "Epoch: [2][400/3750]\tLoss 255.0396\tPrec@1 79.442\n",
      "Epoch: [2][500/3750]\tLoss 237.1745\tPrec@1 80.308\n",
      "Epoch: [2][600/3750]\tLoss 223.3783\tPrec@1 80.881\n",
      "Epoch: [2][700/3750]\tLoss 209.8577\tPrec@1 81.357\n",
      "Epoch: [2][800/3750]\tLoss 200.0600\tPrec@1 81.742\n",
      "Epoch: [2][900/3750]\tLoss 192.7545\tPrec@1 82.034\n",
      "Epoch: [2][1000/3750]\tLoss 185.6904\tPrec@1 82.286\n",
      "Epoch: [2][1100/3750]\tLoss 180.0099\tPrec@1 82.558\n",
      "Epoch: [2][1200/3750]\tLoss 175.6412\tPrec@1 82.621\n",
      "Epoch: [2][1300/3750]\tLoss 170.9350\tPrec@1 82.811\n",
      "Epoch: [2][1400/3750]\tLoss 167.0581\tPrec@1 82.872\n",
      "Epoch: [2][1500/3750]\tLoss 163.4398\tPrec@1 82.976\n",
      "Epoch: [2][1600/3750]\tLoss 160.3884\tPrec@1 83.003\n",
      "Epoch: [2][1700/3750]\tLoss 156.4287\tPrec@1 83.061\n",
      "Epoch: [2][1800/3750]\tLoss 152.9659\tPrec@1 83.162\n",
      "Epoch: [2][1900/3750]\tLoss 150.8184\tPrec@1 83.101\n",
      "Epoch: [2][2000/3750]\tLoss 148.3324\tPrec@1 83.027\n",
      "Epoch: [2][2100/3750]\tLoss 145.0761\tPrec@1 83.130\n",
      "Epoch: [2][2200/3750]\tLoss 142.0692\tPrec@1 83.191\n",
      "Epoch: [2][2300/3750]\tLoss 139.0762\tPrec@1 83.255\n",
      "Epoch: [2][2400/3750]\tLoss 135.7976\tPrec@1 83.372\n",
      "Epoch: [2][2500/3750]\tLoss 131.8983\tPrec@1 83.588\n",
      "Epoch: [2][2600/3750]\tLoss 128.9084\tPrec@1 83.732\n",
      "Epoch: [2][2700/3750]\tLoss 126.2014\tPrec@1 83.768\n",
      "Epoch: [2][2800/3750]\tLoss 123.3785\tPrec@1 83.793\n",
      "Epoch: [2][2900/3750]\tLoss 120.3188\tPrec@1 83.853\n",
      "Epoch: [2][3000/3750]\tLoss 117.4716\tPrec@1 83.855\n",
      "Epoch: [2][3100/3750]\tLoss 114.5448\tPrec@1 83.823\n",
      "Epoch: [2][3200/3750]\tLoss 111.6044\tPrec@1 83.713\n",
      "Epoch: [2][3300/3750]\tLoss 108.6124\tPrec@1 83.550\n",
      "Epoch: [2][3400/3750]\tLoss 105.6519\tPrec@1 83.311\n",
      "Epoch: [2][3500/3750]\tLoss 102.7883\tPrec@1 82.997\n",
      "Epoch: [2][3600/3750]\tLoss 100.0752\tPrec@1 82.645\n",
      "Epoch: [2][3700/3750]\tLoss 97.4492\tPrec@1 82.227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████▉                              | 3/10 [20:50<49:33, 424.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[2] *Validation*: Prec@1 52.158\n",
      "current lr 1.00000e-04\n",
      "tensor([2.4074], device='cuda:0')\n",
      "Epoch: [3][0/3750]\tLoss 267.8614\tPrec@1 34.375\n",
      "Epoch: [3][100/3750]\tLoss 103.0750\tPrec@1 58.106\n",
      "Epoch: [3][200/3750]\tLoss 55.5928\tPrec@1 50.342\n",
      "Epoch: [3][300/3750]\tLoss 38.4528\tPrec@1 45.089\n",
      "Epoch: [3][400/3750]\tLoss 29.4529\tPrec@1 41.474\n",
      "Epoch: [3][500/3750]\tLoss 24.0016\tPrec@1 40.488\n",
      "Epoch: [3][600/3750]\tLoss 20.2789\tPrec@1 39.351\n",
      "Epoch: [3][700/3750]\tLoss 17.6420\tPrec@1 38.392\n",
      "Epoch: [3][800/3750]\tLoss 15.6269\tPrec@1 37.769\n",
      "Epoch: [3][900/3750]\tLoss 14.0879\tPrec@1 37.181\n",
      "Epoch: [3][1000/3750]\tLoss 12.8298\tPrec@1 36.754\n",
      "Epoch: [3][1100/3750]\tLoss 11.8146\tPrec@1 36.521\n",
      "Epoch: [3][1200/3750]\tLoss 10.9545\tPrec@1 36.111\n",
      "Epoch: [3][1300/3750]\tLoss 10.2446\tPrec@1 35.862\n",
      "Epoch: [3][1400/3750]\tLoss 9.6400\tPrec@1 35.343\n",
      "Epoch: [3][1500/3750]\tLoss 9.0926\tPrec@1 34.943\n",
      "Epoch: [3][1600/3750]\tLoss 8.6199\tPrec@1 34.717\n",
      "Epoch: [3][1700/3750]\tLoss 8.2048\tPrec@1 34.498\n",
      "Epoch: [3][1800/3750]\tLoss 7.8399\tPrec@1 34.427\n",
      "Epoch: [3][1900/3750]\tLoss 7.5073\tPrec@1 34.100\n",
      "Epoch: [3][2000/3750]\tLoss 7.2008\tPrec@1 33.961\n",
      "Epoch: [3][2100/3750]\tLoss 6.9212\tPrec@1 33.926\n",
      "Epoch: [3][2200/3750]\tLoss 6.6712\tPrec@1 34.142\n",
      "Epoch: [3][2300/3750]\tLoss 6.4559\tPrec@1 34.296\n",
      "Epoch: [3][2400/3750]\tLoss 6.2453\tPrec@1 34.233\n",
      "Epoch: [3][2500/3750]\tLoss 6.0470\tPrec@1 34.256\n",
      "Epoch: [3][2600/3750]\tLoss 5.8858\tPrec@1 34.236\n",
      "Epoch: [3][2700/3750]\tLoss 5.7221\tPrec@1 34.278\n",
      "Epoch: [3][2800/3750]\tLoss 5.5773\tPrec@1 34.029\n",
      "Epoch: [3][2900/3750]\tLoss 5.4321\tPrec@1 33.942\n",
      "Epoch: [3][3000/3750]\tLoss 5.2993\tPrec@1 34.062\n",
      "Epoch: [3][3100/3750]\tLoss 5.1725\tPrec@1 34.152\n",
      "Epoch: [3][3200/3750]\tLoss 5.0514\tPrec@1 34.282\n",
      "Epoch: [3][3300/3750]\tLoss 4.9389\tPrec@1 34.498\n",
      "Epoch: [3][3400/3750]\tLoss 4.8418\tPrec@1 34.411\n",
      "Epoch: [3][3500/3750]\tLoss 4.7500\tPrec@1 34.359\n",
      "Epoch: [3][3600/3750]\tLoss 4.6626\tPrec@1 34.445\n",
      "Epoch: [3][3700/3750]\tLoss 4.5728\tPrec@1 34.340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████▏                         | 4/10 [28:10<43:04, 430.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3] *Validation*: Prec@1 28.382\n",
      "current lr 1.00000e-04\n",
      "tensor([6.9513], device='cuda:0')\n",
      "Epoch: [4][0/3750]\tLoss 1.4282\tPrec@1 9.375\n",
      "Epoch: [4][100/3750]\tLoss 4.6960\tPrec@1 25.959\n",
      "Epoch: [4][200/3750]\tLoss 3.8146\tPrec@1 27.550\n",
      "Epoch: [4][300/3750]\tLoss 3.1521\tPrec@1 27.014\n",
      "Epoch: [4][400/3750]\tLoss 2.7243\tPrec@1 26.333\n",
      "Epoch: [4][500/3750]\tLoss 2.5644\tPrec@1 26.896\n",
      "Epoch: [4][600/3750]\tLoss 2.3718\tPrec@1 26.976\n",
      "Epoch: [4][700/3750]\tLoss 2.2322\tPrec@1 26.725\n",
      "Epoch: [4][800/3750]\tLoss 2.1280\tPrec@1 26.923\n",
      "Epoch: [4][900/3750]\tLoss 2.1315\tPrec@1 26.276\n",
      "Epoch: [4][1000/3750]\tLoss 2.0740\tPrec@1 26.271\n",
      "Epoch: [4][1100/3750]\tLoss 2.0186\tPrec@1 26.229\n",
      "Epoch: [4][1200/3750]\tLoss 1.9735\tPrec@1 25.687\n",
      "Epoch: [4][1300/3750]\tLoss 1.9524\tPrec@1 25.994\n",
      "Epoch: [4][1400/3750]\tLoss 1.9134\tPrec@1 25.714\n",
      "Epoch: [4][1500/3750]\tLoss 1.8867\tPrec@1 25.783\n",
      "Epoch: [4][1600/3750]\tLoss 1.8553\tPrec@1 25.705\n",
      "Epoch: [4][1700/3750]\tLoss 1.8274\tPrec@1 25.581\n",
      "Epoch: [4][1800/3750]\tLoss 1.8030\tPrec@1 25.783\n",
      "Epoch: [4][1900/3750]\tLoss 1.7964\tPrec@1 25.570\n",
      "Epoch: [4][2000/3750]\tLoss 1.7789\tPrec@1 25.637\n",
      "Epoch: [4][2100/3750]\tLoss 1.7787\tPrec@1 25.513\n",
      "Epoch: [4][2200/3750]\tLoss 1.7650\tPrec@1 25.601\n",
      "Epoch: [4][2300/3750]\tLoss 1.7509\tPrec@1 25.507\n",
      "Epoch: [4][2400/3750]\tLoss 1.7357\tPrec@1 25.430\n",
      "Epoch: [4][2500/3750]\tLoss 1.7227\tPrec@1 25.546\n",
      "Epoch: [4][2600/3750]\tLoss 1.7098\tPrec@1 25.563\n",
      "Epoch: [4][2700/3750]\tLoss 1.7055\tPrec@1 25.585\n",
      "Epoch: [4][2800/3750]\tLoss 1.6941\tPrec@1 25.466\n",
      "Epoch: [4][2900/3750]\tLoss 1.6918\tPrec@1 25.491\n",
      "Epoch: [4][3000/3750]\tLoss 1.6865\tPrec@1 25.436\n",
      "Epoch: [4][3100/3750]\tLoss 1.6872\tPrec@1 25.541\n",
      "Epoch: [4][3200/3750]\tLoss 1.6808\tPrec@1 25.570\n",
      "Epoch: [4][3300/3750]\tLoss 1.6819\tPrec@1 25.680\n",
      "Epoch: [4][3400/3750]\tLoss 1.6732\tPrec@1 25.634\n",
      "Epoch: [4][3500/3750]\tLoss 1.6856\tPrec@1 25.616\n",
      "Epoch: [4][3600/3750]\tLoss 1.6792\tPrec@1 25.681\n",
      "Epoch: [4][3700/3750]\tLoss 1.6713\tPrec@1 25.591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████▌                     | 5/10 [35:31<36:11, 434.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[4] *Validation*: Prec@1 25.000\n",
      "current lr 1.00000e-04\n",
      "tensor([20.0717], device='cuda:0')\n",
      "Epoch: [5][0/3750]\tLoss 1.3742\tPrec@1 0.000\n",
      "Epoch: [5][100/3750]\tLoss 2.1150\tPrec@1 19.183\n",
      "Epoch: [5][200/3750]\tLoss 2.0080\tPrec@1 22.870\n",
      "Epoch: [5][300/3750]\tLoss 1.8518\tPrec@1 23.630\n",
      "Epoch: [5][400/3750]\tLoss 1.8104\tPrec@1 23.582\n",
      "Epoch: [5][500/3750]\tLoss 1.7702\tPrec@1 24.538\n",
      "Epoch: [5][600/3750]\tLoss 1.7734\tPrec@1 24.839\n",
      "Epoch: [5][700/3750]\tLoss 1.7440\tPrec@1 24.657\n",
      "Epoch: [5][800/3750]\tLoss 1.6994\tPrec@1 25.066\n",
      "Epoch: [5][900/3750]\tLoss 1.6648\tPrec@1 24.587\n",
      "Epoch: [5][1000/3750]\tLoss 1.6370\tPrec@1 24.719\n",
      "Epoch: [5][1100/3750]\tLoss 1.6142\tPrec@1 24.813\n",
      "Epoch: [5][1200/3750]\tLoss 1.5953\tPrec@1 24.381\n",
      "Epoch: [5][1300/3750]\tLoss 1.5887\tPrec@1 24.772\n",
      "Epoch: [5][1400/3750]\tLoss 1.5744\tPrec@1 24.547\n",
      "Epoch: [5][1500/3750]\tLoss 1.5618\tPrec@1 24.617\n",
      "Epoch: [5][1600/3750]\tLoss 1.5556\tPrec@1 24.502\n",
      "Epoch: [5][1700/3750]\tLoss 1.5456\tPrec@1 24.392\n",
      "Epoch: [5][1800/3750]\tLoss 1.5367\tPrec@1 24.653\n",
      "Epoch: [5][1900/3750]\tLoss 1.5289\tPrec@1 24.494\n",
      "Epoch: [5][2000/3750]\tLoss 1.5241\tPrec@1 24.605\n",
      "Epoch: [5][2100/3750]\tLoss 1.5176\tPrec@1 24.521\n",
      "Epoch: [5][2200/3750]\tLoss 1.5116\tPrec@1 24.641\n",
      "Epoch: [5][2300/3750]\tLoss 1.5068\tPrec@1 24.586\n",
      "Epoch: [5][2400/3750]\tLoss 1.5018\tPrec@1 24.539\n",
      "Epoch: [5][2500/3750]\tLoss 1.5030\tPrec@1 24.683\n",
      "Epoch: [5][2600/3750]\tLoss 1.5366\tPrec@1 24.731\n",
      "Epoch: [5][2700/3750]\tLoss 1.5310\tPrec@1 24.782\n",
      "Epoch: [5][2800/3750]\tLoss 1.5412\tPrec@1 24.688\n",
      "Epoch: [5][2900/3750]\tLoss 1.5361\tPrec@1 24.738\n",
      "Epoch: [5][3000/3750]\tLoss 1.5318\tPrec@1 24.707\n",
      "Epoch: [5][3100/3750]\tLoss 1.5280\tPrec@1 24.835\n",
      "Epoch: [5][3200/3750]\tLoss 1.5304\tPrec@1 24.884\n",
      "Epoch: [5][3300/3750]\tLoss 1.5260\tPrec@1 25.014\n",
      "Epoch: [5][3400/3750]\tLoss 1.5219\tPrec@1 24.987\n",
      "Epoch: [5][3500/3750]\tLoss 1.5180\tPrec@1 24.985\n",
      "Epoch: [5][3600/3750]\tLoss 1.5143\tPrec@1 25.068\n",
      "Epoch: [5][3700/3750]\tLoss 1.5109\tPrec@1 24.993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████▊                 | 6/10 [42:52<29:06, 436.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[5] *Validation*: Prec@1 25.000\n",
      "current lr 1.00000e-04\n",
      "tensor([57.9565], device='cuda:0')\n",
      "Epoch: [6][0/3750]\tLoss 1.3754\tPrec@1 0.000\n",
      "Epoch: [6][100/3750]\tLoss 1.3875\tPrec@1 19.400\n",
      "Epoch: [6][200/3750]\tLoss 1.3878\tPrec@1 21.953\n",
      "Epoch: [6][300/3750]\tLoss 1.5044\tPrec@1 22.913\n",
      "Epoch: [6][400/3750]\tLoss 1.4749\tPrec@1 23.543\n",
      "Epoch: [6][500/3750]\tLoss 1.4881\tPrec@1 23.310\n",
      "Epoch: [6][600/3750]\tLoss 1.4713\tPrec@1 23.804\n",
      "Epoch: [6][700/3750]\tLoss 1.4644\tPrec@1 23.747\n",
      "Epoch: [6][800/3750]\tLoss 1.4546\tPrec@1 24.270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████▊                 | 6/10 [44:35<29:43, 445.97s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5665/3461410955.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner_ag_news\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Transformer-Quantization/utils/training_ema.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Transformer-Quantization/utils/training_ema.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, epoch, verbose)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;31m# measure accuracy and record loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mprec1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0mtop1_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprec1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learner_ag_news.train()"
   ]
>>>>>>> Stashed changes
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}