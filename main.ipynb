{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "071a4240",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train transformer"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 4,
>>>>>>> Stashed changes
   "id": "5d29b0aa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from utils.data_utils import AG_NEWS_DATASET\n",
    "from utils.constants import *\n",
    "from utils.training import Learner\n",
    "\n",
    "from quantization.binarize import binarize, IRLinear\n",
    "from quantization.transformer import Transformer\n",
    "from quantization.quantize import quantizer\n",
    "from quantization.pytorch_api import ModelQuant\n",
    "from quantization.fully_quantize import Model as fullyQuantModel\n",
    "\n",
    "from utils.train_utils import change_t\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 5,
>>>>>>> Stashed changes
   "id": "49ac3637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, quant_type=None, quant_method=None, bit_num=None, quant_pattern=None):\n",
    "    '''\n",
    "    Create training model based on sepcified quant_type\n",
    "    ----------\n",
    "    Arguments:\n",
    "    quant_type    - quant type, should be one of [None, 'quantization', 'binarization']\n",
    "    quant_method  - quant method to use, if quant_type is None, it should also be None\n",
    "                    For 'quantization', should be one of ['basic', 'pytorch', 'fully']\n",
    "                    For 'binarization', should be one of ['basic', 'ir']\n",
    "    bit_num       - bit number for each parameter, only works when quant_type is 'quantization'\n",
    "                    should be one of [8,4,2]\n",
    "    quant_pattern - quantization pattern, should be one of ['MHA', 'FFN', 'CLS', 'ALL']\n",
    "    '''\n",
    "    model = Transformer(d_model=BASELINE_MODEL_DIM,\n",
    "                             d_ff=BASELINE_FFN_DIM,\n",
    "                             d_hidden=BASELINE_HIDDEN_DIM,\n",
    "                             h=BASELINE_MODEL_NUMBER_OF_HEADS,\n",
    "                             n_layers=BASELINE_MODEL_NUMBER_OF_LAYERS,\n",
    "                             n_class=4,\n",
    "                             vocab=vocab_size\n",
    "                            )\n",
    "    \n",
    "    __quant_type__ = [None,'quantization','binarization']\n",
    "    __bit_num__ = [None,8,4,2]\n",
    "    __quant_pattern__ = [None,'MHA', 'FFN', 'CLS', 'ALL']\n",
    "    \n",
    "    assert quant_type in __quant_type__, f\"Unimplemented quantization type, should be one of {__quant_type__}, got '{quant_type}'!\"\n",
    "    assert bit_num in __bit_num__, f\"Unimplemented bit number, should be one of {__bit_num__}, got '{bit_num}'!\"\n",
    "    assert quant_pattern in __quant_pattern__, f\"Unimplemented quantization method, should be one of {__quant_pattern__}, got '{quant_pattern}'!\"\n",
    "    \n",
    "    if quant_type == None:\n",
    "        if quant_method is not None:\n",
    "            print(f\"Quant method {quant_method} will not work in baseline model!\")\n",
    "        if bit_num is not None:\n",
    "            print(f\"Bit number {bit_num} will not work in baseline model!\")\n",
    "        if quant_pattern is not None:\n",
    "            print(f\"Quant pattern {quant_pattern} will not work in baseline model!\")\n",
    "    \n",
    "    elif quant_type == 'quantization':\n",
    "        __quant_method__ = ['basic', 'pytorch', 'fully']\n",
    "        \n",
    "        assert quant_method in __quant_method__, f\"Unimplemented quantization method, should be one of {__quant_method__}, got '{quant_method}'!\"\n",
    "        assert bit_num != None, f\"Bit number can not be None!\"\n",
    "        assert quant_pattern != None, f\"Quant pattern can not be None!\"\n",
    "        \n",
    "        if quant_method == 'basic':\n",
    "            if quant_pattern != 'ALL':\n",
    "                print(f\"Current quant method {quant_method} can only quantize the whole network, quant pattern {quant_pattern} will not work!\")\n",
    "            model = quantizer(model, bit_num, True)\n",
    "            \n",
    "        elif quant_method == 'pytorch':\n",
    "            model = ModelQuant(d_model=BASELINE_MODEL_DIM,\n",
    "                               d_ff=BASELINE_FFN_DIM,\n",
    "                               d_hidden=BASELINE_HIDDEN_DIM,\n",
    "                               h=BASELINE_MODEL_NUMBER_OF_HEADS,\n",
    "                               n_layers=BASELINE_MODEL_NUMBER_OF_LAYERS,\n",
    "                               n_class=4,\n",
    "                               vocab=tokenizer.vocab_size,\n",
    "                               quant_ffn=((quant_pattern == 'FFN')|(quant_pattern == 'ALL')),\n",
    "                               quant_mha=((quant_pattern == 'MHA')|(quant_pattern == 'ALL')),\n",
    "                               quant_classifier=((quant_pattern == 'CLS')|(quant_pattern == 'ALL')),\n",
    "                               bit_num=bit_num)\n",
    "            \n",
    "        elif quant_method == 'fully':\n",
    "            print(\"For fully_quantized model, bit number and quant pattern will not work!\")\n",
    "            model = fullyQuantModel(4,\n",
    "                tokenizer.vocab_size,\n",
    "                BASELINE_MODEL_NUMBER_OF_LAYERS,\n",
    "                BASELINE_MODEL_NUMBER_OF_HEADS,\n",
    "                BASELINE_MODEL_DIM)\n",
    "            \n",
    "    elif quant_type == 'binarization':\n",
    "        __quant_method__ = ['basic', 'ir']\n",
    "        assert quant_method in __quant_method__, f\"Unimplemented quantization method, should be one of {__quant_method__}, got '{quant_method}'!\"\n",
    "        assert quant_pattern != None, f\"Quant pattern can not be None!\"\n",
    "        print(f\"For binarization model, bit num will not work!\")\n",
    "        \n",
    "        binarize(model, quant_pattern, skip_final=True, qk_only=True)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 6,
>>>>>>> Stashed changes
   "id": "d3f25f80",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
<<<<<<< Updated upstream
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/datapipes/utils/common.py:25: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
      "  \"Lambda function is not supported for pickle, please use \"\n",
      "/opt/conda/lib/python3.7/site-packages/torchdata/datapipes/map/util/utils.py:78: UserWarning: Data from prior DataPipe are loaded to get length ofIterToMapConverter before execution of the pipeline.Please consider removing len().\n",
      "  \"Data from prior DataPipe are loaded to get length of\"\n"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "# load dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_dl, test_dl = AG_NEWS_DATASET(tokenizer, batch_size = BATCH_SIZE).load_data()\n",
    "\n",
    "# create model\n",
    "model = Transformer(d_model=BASELINE_MODEL_DIM,\n",
    "                    d_ff=BASELINE_FFN_DIM,\n",
    "                    d_hidden=BASELINE_HIDDEN_DIM,\n",
    "                    h=BASELINE_MODEL_NUMBER_OF_HEADS,\n",
    "                    n_layers=BASELINE_MODEL_NUMBER_OF_LAYERS,\n",
    "                    n_class=4,\n",
    "                    vocab=tokenizer.vocab_size\n",
    "                   )\n",
    "\n",
    "# binarize(model, 'ALL', binarize_layer='basic', skip_final=True, qk_only=False)\n",
    "# print(model)\n",
    "\n",
    "# loss func\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# baseline training config -> do not change!\n",
    "optim = Adam(model.parameters(), lr= 1e-4)\n",
    "scheduler = MultiStepLR(optim, milestones=[10,15], gamma=0.1)\n",
    "\n",
    "train_config ={'model': model,\n",
    "               'loss_fn': loss_fn,\n",
    "               'optim': optim,\n",
    "               'scheduler': scheduler,\n",
    "               'datasets': [train_dl, test_dl],\n",
    "               'epochs': 10,\n",
    "               'batch_size': BATCH_SIZE\n",
    "               }\n",
    "\n",
    "train_config['exp_name'] = 'transformer_baseline'\n",
    "\n",
    "# training\n",
    "learner_ag_news = Learner(train_config, ir = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7321ebf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
<<<<<<< Updated upstream
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current lr 1.00000e-04\n",
      "Epoch: [0][0/3750]\tLoss 2.6483\tPrec@1 31.250\n",
      "Epoch: [0][100/3750]\tLoss 1.5789\tPrec@1 27.197\n",
      "Epoch: [0][200/3750]\tLoss 1.4578\tPrec@1 32.074\n",
      "Epoch: [0][300/3750]\tLoss 1.2692\tPrec@1 42.546\n",
      "Epoch: [0][400/3750]\tLoss 1.0944\tPrec@1 51.520\n",
      "Epoch: [0][500/3750]\tLoss 0.9739\tPrec@1 57.747\n",
      "Epoch: [0][600/3750]\tLoss 0.8867\tPrec@1 62.198\n",
      "Epoch: [0][700/3750]\tLoss 0.8159\tPrec@1 65.710\n",
      "Epoch: [0][800/3750]\tLoss 0.7590\tPrec@1 68.469\n",
      "Epoch: [0][900/3750]\tLoss 0.7162\tPrec@1 70.564\n",
      "Epoch: [0][1000/3750]\tLoss 0.6794\tPrec@1 72.237\n",
      "Epoch: [0][1100/3750]\tLoss 0.6488\tPrec@1 73.706\n",
      "Epoch: [0][1200/3750]\tLoss 0.6214\tPrec@1 75.008\n",
      "Epoch: [0][1300/3750]\tLoss 0.5985\tPrec@1 76.054\n",
      "Epoch: [0][1400/3750]\tLoss 0.5775\tPrec@1 77.001\n",
      "Epoch: [0][1500/3750]\tLoss 0.5591\tPrec@1 77.831\n",
      "Epoch: [0][1600/3750]\tLoss 0.5445\tPrec@1 78.539\n",
      "Epoch: [0][1700/3750]\tLoss 0.5291\tPrec@1 79.238\n",
      "Epoch: [0][1800/3750]\tLoss 0.5157\tPrec@1 79.855\n",
      "Epoch: [0][1900/3750]\tLoss 0.5033\tPrec@1 80.418\n",
      "Epoch: [0][2000/3750]\tLoss 0.4925\tPrec@1 80.922\n",
      "Epoch: [0][2100/3750]\tLoss 0.4823\tPrec@1 81.379\n",
      "Epoch: [0][2200/3750]\tLoss 0.4730\tPrec@1 81.794\n",
      "Epoch: [0][2300/3750]\tLoss 0.4636\tPrec@1 82.206\n",
      "Epoch: [0][2400/3750]\tLoss 0.4554\tPrec@1 82.559\n",
      "Epoch: [0][2500/3750]\tLoss 0.4494\tPrec@1 82.843\n",
      "Epoch: [0][2600/3750]\tLoss 0.4423\tPrec@1 83.164\n",
      "Epoch: [0][2700/3750]\tLoss 0.4364\tPrec@1 83.422\n",
      "Epoch: [0][2800/3750]\tLoss 0.4304\tPrec@1 83.693\n",
      "Epoch: [0][2900/3750]\tLoss 0.4249\tPrec@1 83.941\n",
      "Epoch: [0][3000/3750]\tLoss 0.4182\tPrec@1 84.200\n",
      "Epoch: [0][3100/3750]\tLoss 0.4139\tPrec@1 84.394\n",
      "Epoch: [0][3200/3750]\tLoss 0.4096\tPrec@1 84.584\n",
      "Epoch: [0][3300/3750]\tLoss 0.4050\tPrec@1 84.782\n",
      "Epoch: [0][3400/3750]\tLoss 0.4005\tPrec@1 84.992\n",
      "Epoch: [0][3500/3750]\tLoss 0.3974\tPrec@1 85.127\n",
      "Epoch: [0][3600/3750]\tLoss 0.3936\tPrec@1 85.304\n",
      "Epoch: [0][3700/3750]\tLoss 0.3903\tPrec@1 85.464\n",
      "Epoch[0] *Validation*: Prec@1 91.145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▍                                       | 1/10 [01:23<12:34, 83.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current lr 1.00000e-04\n",
      "Epoch: [1][0/3750]\tLoss 0.1058\tPrec@1 96.875\n",
      "Epoch: [1][100/3750]\tLoss 0.1898\tPrec@1 93.038\n",
      "Epoch: [1][200/3750]\tLoss 0.1854\tPrec@1 93.424\n",
      "Epoch: [1][300/3750]\tLoss 0.1907\tPrec@1 93.449\n",
      "Epoch: [1][400/3750]\tLoss 0.1884\tPrec@1 93.493\n",
      "Epoch: [1][500/3750]\tLoss 0.1889\tPrec@1 93.557\n",
      "Epoch: [1][600/3750]\tLoss 0.1882\tPrec@1 93.552\n",
      "Epoch: [1][700/3750]\tLoss 0.1902\tPrec@1 93.514\n",
      "Epoch: [1][800/3750]\tLoss 0.1933\tPrec@1 93.391\n",
      "Epoch: [1][900/3750]\tLoss 0.1923\tPrec@1 93.362\n",
      "Epoch: [1][1000/3750]\tLoss 0.1916\tPrec@1 93.378\n",
      "Epoch: [1][1100/3750]\tLoss 0.1910\tPrec@1 93.426\n",
      "Epoch: [1][1200/3750]\tLoss 0.1924\tPrec@1 93.370\n",
      "Epoch: [1][1300/3750]\tLoss 0.1920\tPrec@1 93.375\n",
      "Epoch: [1][1400/3750]\tLoss 0.1922\tPrec@1 93.362\n",
      "Epoch: [1][1500/3750]\tLoss 0.1928\tPrec@1 93.315\n",
      "Epoch: [1][1600/3750]\tLoss 0.1929\tPrec@1 93.328\n",
      "Epoch: [1][1700/3750]\tLoss 0.1937\tPrec@1 93.276\n",
      "Epoch: [1][1800/3750]\tLoss 0.1940\tPrec@1 93.254\n",
      "Epoch: [1][1900/3750]\tLoss 0.1948\tPrec@1 93.237\n",
      "Epoch: [1][2000/3750]\tLoss 0.1951\tPrec@1 93.217\n",
      "Epoch: [1][2100/3750]\tLoss 0.1948\tPrec@1 93.234\n",
      "Epoch: [1][2200/3750]\tLoss 0.1950\tPrec@1 93.253\n",
      "Epoch: [1][2300/3750]\tLoss 0.1948\tPrec@1 93.283\n",
      "Epoch: [1][2400/3750]\tLoss 0.1961\tPrec@1 93.231\n",
      "Epoch: [1][2500/3750]\tLoss 0.1957\tPrec@1 93.249\n",
      "Epoch: [1][2600/3750]\tLoss 0.1952\tPrec@1 93.273\n",
      "Epoch: [1][2700/3750]\tLoss 0.1947\tPrec@1 93.293\n",
      "Epoch: [1][2800/3750]\tLoss 0.1953\tPrec@1 93.257\n",
      "Epoch: [1][2900/3750]\tLoss 0.1954\tPrec@1 93.271\n",
      "Epoch: [1][3000/3750]\tLoss 0.1961\tPrec@1 93.234\n",
      "Epoch: [1][3100/3750]\tLoss 0.1959\tPrec@1 93.228\n",
      "Epoch: [1][3200/3750]\tLoss 0.1960\tPrec@1 93.234\n",
      "Epoch: [1][3300/3750]\tLoss 0.1963\tPrec@1 93.228\n",
      "Epoch: [1][3400/3750]\tLoss 0.1967\tPrec@1 93.215\n",
      "Epoch: [1][3500/3750]\tLoss 0.1971\tPrec@1 93.197\n",
      "Epoch: [1][3600/3750]\tLoss 0.1969\tPrec@1 93.198\n",
      "Epoch: [1][3700/3750]\tLoss 0.1968\tPrec@1 93.216\n",
      "Epoch[1] *Validation*: Prec@1 91.184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████▊                                   | 2/10 [02:47<11:08, 83.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current lr 1.00000e-04\n",
      "Epoch: [2][0/3750]\tLoss 0.1981\tPrec@1 96.875\n",
      "Epoch: [2][100/3750]\tLoss 0.1245\tPrec@1 95.575\n",
      "Epoch: [2][200/3750]\tLoss 0.1304\tPrec@1 95.211\n",
      "Epoch: [2][300/3750]\tLoss 0.1313\tPrec@1 95.297\n",
      "Epoch: [2][400/3750]\tLoss 0.1298\tPrec@1 95.355\n",
      "Epoch: [2][500/3750]\tLoss 0.1290\tPrec@1 95.397\n",
      "Epoch: [2][600/3750]\tLoss 0.1319\tPrec@1 95.258\n",
      "Epoch: [2][700/3750]\tLoss 0.1306\tPrec@1 95.310\n",
      "Epoch: [2][800/3750]\tLoss 0.1302\tPrec@1 95.361\n",
      "Epoch: [2][900/3750]\tLoss 0.1327\tPrec@1 95.255\n",
      "Epoch: [2][1000/3750]\tLoss 0.1335\tPrec@1 95.236\n",
      "Epoch: [2][1100/3750]\tLoss 0.1334\tPrec@1 95.243\n",
      "Epoch: [2][1200/3750]\tLoss 0.1344\tPrec@1 95.197\n",
      "Epoch: [2][1300/3750]\tLoss 0.1344\tPrec@1 95.184\n",
      "Epoch: [2][1400/3750]\tLoss 0.1340\tPrec@1 95.224\n",
      "Epoch: [2][1500/3750]\tLoss 0.1342\tPrec@1 95.230\n",
      "Epoch: [2][1600/3750]\tLoss 0.1350\tPrec@1 95.192\n",
      "Epoch: [2][1700/3750]\tLoss 0.1347\tPrec@1 95.201\n",
      "Epoch: [2][1800/3750]\tLoss 0.1357\tPrec@1 95.188\n",
      "Epoch: [2][1900/3750]\tLoss 0.1365\tPrec@1 95.152\n",
      "Epoch: [2][2000/3750]\tLoss 0.1381\tPrec@1 95.098\n",
      "Epoch: [2][2100/3750]\tLoss 0.1372\tPrec@1 95.124\n",
      "Epoch: [2][2200/3750]\tLoss 0.1373\tPrec@1 95.130\n",
      "Epoch: [2][2300/3750]\tLoss 0.1377\tPrec@1 95.124\n",
      "Epoch: [2][2400/3750]\tLoss 0.1375\tPrec@1 95.132\n",
      "Epoch: [2][2500/3750]\tLoss 0.1380\tPrec@1 95.129\n",
      "Epoch: [2][2600/3750]\tLoss 0.1382\tPrec@1 95.123\n",
      "Epoch: [2][2700/3750]\tLoss 0.1387\tPrec@1 95.101\n",
      "Epoch: [2][2800/3750]\tLoss 0.1396\tPrec@1 95.084\n",
      "Epoch: [2][2900/3750]\tLoss 0.1397\tPrec@1 95.087\n",
      "Epoch: [2][3000/3750]\tLoss 0.1396\tPrec@1 95.094\n",
      "Epoch: [2][3100/3750]\tLoss 0.1399\tPrec@1 95.091\n",
      "Epoch: [2][3200/3750]\tLoss 0.1400\tPrec@1 95.081\n",
      "Epoch: [2][3300/3750]\tLoss 0.1400\tPrec@1 95.077\n",
      "Epoch: [2][3400/3750]\tLoss 0.1396\tPrec@1 95.090\n",
      "Epoch: [2][3500/3750]\tLoss 0.1398\tPrec@1 95.086\n",
      "Epoch: [2][3600/3750]\tLoss 0.1399\tPrec@1 95.070\n",
      "Epoch: [2][3700/3750]\tLoss 0.1401\tPrec@1 95.055\n",
      "Epoch[2] *Validation*: Prec@1 91.724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|█████████████▏                              | 3/10 [04:10<09:44, 83.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current lr 1.00000e-04\n",
      "Epoch: [3][0/3750]\tLoss 0.0744\tPrec@1 96.875\n",
      "Epoch: [3][100/3750]\tLoss 0.0682\tPrec@1 97.710\n",
      "Epoch: [3][200/3750]\tLoss 0.0739\tPrec@1 97.419\n",
      "Epoch: [3][300/3750]\tLoss 0.0754\tPrec@1 97.353\n",
      "Epoch: [3][400/3750]\tLoss 0.0793\tPrec@1 97.233\n",
      "Epoch: [3][500/3750]\tLoss 0.0788\tPrec@1 97.268\n",
      "Epoch: [3][600/3750]\tLoss 0.0838\tPrec@1 97.093\n",
      "Epoch: [3][700/3750]\tLoss 0.0838\tPrec@1 97.120\n",
      "Epoch: [3][800/3750]\tLoss 0.0848\tPrec@1 97.058\n",
      "Epoch: [3][900/3750]\tLoss 0.0858\tPrec@1 97.038\n",
      "Epoch: [3][1000/3750]\tLoss 0.0866\tPrec@1 96.975\n",
      "Epoch: [3][1100/3750]\tLoss 0.0864\tPrec@1 96.932\n",
      "Epoch: [3][1200/3750]\tLoss 0.0875\tPrec@1 96.859\n",
      "Epoch: [3][1300/3750]\tLoss 0.0884\tPrec@1 96.829\n",
      "Epoch: [3][1400/3750]\tLoss 0.0886\tPrec@1 96.819\n",
      "Epoch: [3][1500/3750]\tLoss 0.0880\tPrec@1 96.852\n",
      "Epoch: [3][1600/3750]\tLoss 0.0891\tPrec@1 96.822\n",
      "Epoch: [3][1700/3750]\tLoss 0.0895\tPrec@1 96.816\n",
      "Epoch: [3][1800/3750]\tLoss 0.0901\tPrec@1 96.792\n",
      "Epoch: [3][1900/3750]\tLoss 0.0894\tPrec@1 96.798\n",
      "Epoch: [3][2000/3750]\tLoss 0.0896\tPrec@1 96.783\n",
      "Epoch: [3][2100/3750]\tLoss 0.0895\tPrec@1 96.795\n",
      "Epoch: [3][2200/3750]\tLoss 0.0901\tPrec@1 96.783\n",
      "Epoch: [3][2300/3750]\tLoss 0.0905\tPrec@1 96.760\n",
      "Epoch: [3][2400/3750]\tLoss 0.0909\tPrec@1 96.760\n",
      "Epoch: [3][2500/3750]\tLoss 0.0911\tPrec@1 96.761\n",
      "Epoch: [3][2600/3750]\tLoss 0.0907\tPrec@1 96.770\n",
      "Epoch: [3][2700/3750]\tLoss 0.0913\tPrec@1 96.750\n",
      "Epoch: [3][2800/3750]\tLoss 0.0918\tPrec@1 96.734\n",
      "Epoch: [3][2900/3750]\tLoss 0.0919\tPrec@1 96.735\n",
      "Epoch: [3][3000/3750]\tLoss 0.0927\tPrec@1 96.706\n",
      "Epoch: [3][3100/3750]\tLoss 0.0931\tPrec@1 96.684\n",
      "Epoch: [3][3200/3750]\tLoss 0.0932\tPrec@1 96.681\n",
      "Epoch: [3][3300/3750]\tLoss 0.0938\tPrec@1 96.657\n",
      "Epoch: [3][3400/3750]\tLoss 0.0939\tPrec@1 96.650\n",
      "Epoch: [3][3500/3750]\tLoss 0.0944\tPrec@1 96.630\n",
      "Epoch: [3][3600/3750]\tLoss 0.0949\tPrec@1 96.605\n",
      "Epoch: [3][3700/3750]\tLoss 0.0951\tPrec@1 96.584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████▌                          | 4/10 [05:32<08:17, 82.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3] *Validation*: Prec@1 90.829\n",
      "current lr 1.00000e-04\n",
      "Epoch: [4][0/3750]\tLoss 0.0364\tPrec@1 100.000\n",
      "Epoch: [4][100/3750]\tLoss 0.0566\tPrec@1 98.082\n",
      "Epoch: [4][200/3750]\tLoss 0.0549\tPrec@1 98.150\n",
      "Epoch: [4][300/3750]\tLoss 0.0538\tPrec@1 98.090\n",
      "Epoch: [4][400/3750]\tLoss 0.0542\tPrec@1 98.052\n",
      "Epoch: [4][500/3750]\tLoss 0.0546\tPrec@1 98.035\n",
      "Epoch: [4][600/3750]\tLoss 0.0544\tPrec@1 98.024\n",
      "Epoch: [4][700/3750]\tLoss 0.0555\tPrec@1 98.052\n",
      "Epoch: [4][800/3750]\tLoss 0.0565\tPrec@1 98.018\n",
      "Epoch: [4][900/3750]\tLoss 0.0564\tPrec@1 98.040\n",
      "Epoch: [4][1000/3750]\tLoss 0.0570\tPrec@1 98.018\n",
      "Epoch: [4][1100/3750]\tLoss 0.0580\tPrec@1 97.968\n",
      "Epoch: [4][1200/3750]\tLoss 0.0581\tPrec@1 97.970\n",
      "Epoch: [4][1300/3750]\tLoss 0.0577\tPrec@1 97.987\n",
      "Epoch: [4][1400/3750]\tLoss 0.0577\tPrec@1 98.001\n",
      "Epoch: [4][1500/3750]\tLoss 0.0581\tPrec@1 97.997\n",
      "Epoch: [4][1600/3750]\tLoss 0.0578\tPrec@1 97.991\n",
      "Epoch: [4][1700/3750]\tLoss 0.0576\tPrec@1 97.992\n",
      "Epoch: [4][1800/3750]\tLoss 0.0580\tPrec@1 97.979\n",
      "Epoch: [4][1900/3750]\tLoss 0.0581\tPrec@1 97.965\n",
      "Epoch: [4][2000/3750]\tLoss 0.0578\tPrec@1 97.968\n",
      "Epoch: [4][2100/3750]\tLoss 0.0579\tPrec@1 97.968\n",
      "Epoch: [4][2200/3750]\tLoss 0.0585\tPrec@1 97.951\n",
      "Epoch: [4][2300/3750]\tLoss 0.0582\tPrec@1 97.953\n",
      "Epoch: [4][2400/3750]\tLoss 0.0589\tPrec@1 97.921\n",
      "Epoch: [4][2500/3750]\tLoss 0.0592\tPrec@1 97.907\n",
      "Epoch: [4][2600/3750]\tLoss 0.0595\tPrec@1 97.891\n",
      "Epoch: [4][2700/3750]\tLoss 0.0594\tPrec@1 97.892\n",
      "Epoch: [4][2800/3750]\tLoss 0.0600\tPrec@1 97.869\n",
      "Epoch: [4][2900/3750]\tLoss 0.0608\tPrec@1 97.843\n",
      "Epoch: [4][3000/3750]\tLoss 0.0610\tPrec@1 97.835\n",
      "Epoch: [4][3100/3750]\tLoss 0.0614\tPrec@1 97.816\n",
      "Epoch: [4][3200/3750]\tLoss 0.0617\tPrec@1 97.808\n",
      "Epoch: [4][3300/3750]\tLoss 0.0617\tPrec@1 97.802\n",
      "Epoch: [4][3400/3750]\tLoss 0.0617\tPrec@1 97.800\n",
      "Epoch: [4][3500/3750]\tLoss 0.0623\tPrec@1 97.778\n",
      "Epoch: [4][3600/3750]\tLoss 0.0627\tPrec@1 97.769\n",
      "Epoch: [4][3700/3750]\tLoss 0.0632\tPrec@1 97.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████                      | 5/10 [06:54<06:53, 82.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[4] *Validation*: Prec@1 91.553\n",
      "current lr 1.00000e-04\n",
      "Epoch: [5][0/3750]\tLoss 0.0042\tPrec@1 100.000\n",
      "Epoch: [5][100/3750]\tLoss 0.0320\tPrec@1 98.731\n",
      "Epoch: [5][200/3750]\tLoss 0.0347\tPrec@1 98.678\n",
      "Epoch: [5][300/3750]\tLoss 0.0328\tPrec@1 98.775\n",
      "Epoch: [5][400/3750]\tLoss 0.0322\tPrec@1 98.792\n",
      "Epoch: [5][500/3750]\tLoss 0.0316\tPrec@1 98.790\n",
      "Epoch: [5][600/3750]\tLoss 0.0322\tPrec@1 98.778\n",
      "Epoch: [5][700/3750]\tLoss 0.0332\tPrec@1 98.770\n",
      "Epoch: [5][800/3750]\tLoss 0.0333\tPrec@1 98.787\n",
      "Epoch: [5][900/3750]\tLoss 0.0333\tPrec@1 98.772\n",
      "Epoch: [5][1000/3750]\tLoss 0.0328\tPrec@1 98.795\n",
      "Epoch: [5][1100/3750]\tLoss 0.0338\tPrec@1 98.745\n",
      "Epoch: [5][1200/3750]\tLoss 0.0336\tPrec@1 98.767\n",
      "Epoch: [5][1300/3750]\tLoss 0.0352\tPrec@1 98.722\n",
      "Epoch: [5][1400/3750]\tLoss 0.0359\tPrec@1 98.700\n",
      "Epoch: [5][1500/3750]\tLoss 0.0353\tPrec@1 98.726\n",
      "Epoch: [5][1600/3750]\tLoss 0.0358\tPrec@1 98.704\n",
      "Epoch: [5][1700/3750]\tLoss 0.0360\tPrec@1 98.681\n",
      "Epoch: [5][1800/3750]\tLoss 0.0361\tPrec@1 98.667\n",
      "Epoch: [5][1900/3750]\tLoss 0.0360\tPrec@1 98.685\n",
      "Epoch: [5][2000/3750]\tLoss 0.0362\tPrec@1 98.680\n",
      "Epoch: [5][2100/3750]\tLoss 0.0371\tPrec@1 98.657\n",
      "Epoch: [5][2200/3750]\tLoss 0.0373\tPrec@1 98.651\n",
      "Epoch: [5][2300/3750]\tLoss 0.0376\tPrec@1 98.639\n",
      "Epoch: [5][2400/3750]\tLoss 0.0380\tPrec@1 98.637\n",
      "Epoch: [5][2500/3750]\tLoss 0.0379\tPrec@1 98.648\n",
      "Epoch: [5][2600/3750]\tLoss 0.0381\tPrec@1 98.638\n",
      "Epoch: [5][2700/3750]\tLoss 0.0385\tPrec@1 98.624\n",
      "Epoch: [5][2800/3750]\tLoss 0.0388\tPrec@1 98.618\n",
      "Epoch: [5][2900/3750]\tLoss 0.0390\tPrec@1 98.605\n",
      "Epoch: [5][3000/3750]\tLoss 0.0393\tPrec@1 98.598\n",
      "Epoch: [5][3100/3750]\tLoss 0.0394\tPrec@1 98.598\n",
      "Epoch: [5][3200/3750]\tLoss 0.0396\tPrec@1 98.590\n",
      "Epoch: [5][3300/3750]\tLoss 0.0402\tPrec@1 98.567\n",
      "Epoch: [5][3400/3750]\tLoss 0.0409\tPrec@1 98.546\n",
      "Epoch: [5][3500/3750]\tLoss 0.0412\tPrec@1 98.533\n",
      "Epoch: [5][3600/3750]\tLoss 0.0413\tPrec@1 98.527\n",
      "Epoch: [5][3700/3750]\tLoss 0.0414\tPrec@1 98.527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████▍                 | 6/10 [08:17<05:30, 82.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[5] *Validation*: Prec@1 91.000\n",
      "current lr 1.00000e-04\n",
      "Epoch: [6][0/3750]\tLoss 0.0100\tPrec@1 100.000\n",
      "Epoch: [6][100/3750]\tLoss 0.0162\tPrec@1 99.443\n",
      "Epoch: [6][200/3750]\tLoss 0.0180\tPrec@1 99.300\n",
      "Epoch: [6][300/3750]\tLoss 0.0179\tPrec@1 99.263\n",
      "Epoch: [6][400/3750]\tLoss 0.0185\tPrec@1 99.275\n",
      "Epoch: [6][500/3750]\tLoss 0.0195\tPrec@1 99.264\n",
      "Epoch: [6][600/3750]\tLoss 0.0197\tPrec@1 99.272\n",
      "Epoch: [6][700/3750]\tLoss 0.0198\tPrec@1 99.247\n",
      "Epoch: [6][800/3750]\tLoss 0.0206\tPrec@1 99.200\n",
      "Epoch: [6][900/3750]\tLoss 0.0209\tPrec@1 99.199\n",
      "Epoch: [6][1000/3750]\tLoss 0.0215\tPrec@1 99.185\n",
      "Epoch: [6][1100/3750]\tLoss 0.0213\tPrec@1 99.188\n",
      "Epoch: [6][1200/3750]\tLoss 0.0215\tPrec@1 99.193\n",
      "Epoch: [6][1300/3750]\tLoss 0.0222\tPrec@1 99.171\n",
      "Epoch: [6][1400/3750]\tLoss 0.0227\tPrec@1 99.155\n",
      "Epoch: [6][1500/3750]\tLoss 0.0238\tPrec@1 99.136\n",
      "Epoch: [6][1600/3750]\tLoss 0.0242\tPrec@1 99.122\n",
      "Epoch: [6][1700/3750]\tLoss 0.0248\tPrec@1 99.109\n",
      "Epoch: [6][1800/3750]\tLoss 0.0247\tPrec@1 99.108\n",
      "Epoch: [6][1900/3750]\tLoss 0.0249\tPrec@1 99.107\n",
      "Epoch: [6][2000/3750]\tLoss 0.0249\tPrec@1 99.105\n",
      "Epoch: [6][2100/3750]\tLoss 0.0258\tPrec@1 99.072\n",
      "Epoch: [6][2200/3750]\tLoss 0.0259\tPrec@1 99.077\n",
      "Epoch: [6][2300/3750]\tLoss 0.0264\tPrec@1 99.062\n",
      "Epoch: [6][2400/3750]\tLoss 0.0268\tPrec@1 99.046\n",
      "Epoch: [6][2500/3750]\tLoss 0.0276\tPrec@1 99.019\n",
      "Epoch: [6][2600/3750]\tLoss 0.0278\tPrec@1 99.016\n",
      "Epoch: [6][2700/3750]\tLoss 0.0281\tPrec@1 99.006\n",
      "Epoch: [6][2800/3750]\tLoss 0.0282\tPrec@1 98.994\n",
      "Epoch: [6][2900/3750]\tLoss 0.0284\tPrec@1 98.985\n",
      "Epoch: [6][3000/3750]\tLoss 0.0288\tPrec@1 98.968\n",
      "Epoch: [6][3100/3750]\tLoss 0.0288\tPrec@1 98.970\n",
      "Epoch: [6][3200/3750]\tLoss 0.0293\tPrec@1 98.951\n",
      "Epoch: [6][3300/3750]\tLoss 0.0291\tPrec@1 98.957\n",
      "Epoch: [6][3400/3750]\tLoss 0.0292\tPrec@1 98.957\n",
      "Epoch: [6][3500/3750]\tLoss 0.0291\tPrec@1 98.960\n",
      "Epoch: [6][3600/3750]\tLoss 0.0298\tPrec@1 98.934\n",
      "Epoch: [6][3700/3750]\tLoss 0.0301\tPrec@1 98.921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████▊             | 7/10 [09:40<04:08, 82.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[6] *Validation*: Prec@1 91.118\n",
      "current lr 1.00000e-04\n",
      "Epoch: [7][0/3750]\tLoss 0.0005\tPrec@1 100.000\n",
      "Epoch: [7][100/3750]\tLoss 0.0142\tPrec@1 99.505\n",
      "Epoch: [7][200/3750]\tLoss 0.0149\tPrec@1 99.425\n",
      "Epoch: [7][300/3750]\tLoss 0.0140\tPrec@1 99.481\n",
      "Epoch: [7][400/3750]\tLoss 0.0154\tPrec@1 99.454\n",
      "Epoch: [7][500/3750]\tLoss 0.0154\tPrec@1 99.470\n",
      "Epoch: [7][600/3750]\tLoss 0.0157\tPrec@1 99.480\n",
      "Epoch: [7][700/3750]\tLoss 0.0153\tPrec@1 99.501\n",
      "Epoch: [7][800/3750]\tLoss 0.0152\tPrec@1 99.508\n",
      "Epoch: [7][900/3750]\tLoss 0.0158\tPrec@1 99.483\n",
      "Epoch: [7][1000/3750]\tLoss 0.0160\tPrec@1 99.460\n",
      "Epoch: [7][1100/3750]\tLoss 0.0163\tPrec@1 99.441\n",
      "Epoch: [7][1200/3750]\tLoss 0.0175\tPrec@1 99.404\n",
      "Epoch: [7][1300/3750]\tLoss 0.0182\tPrec@1 99.387\n",
      "Epoch: [7][1400/3750]\tLoss 0.0183\tPrec@1 99.391\n",
      "Epoch: [7][1500/3750]\tLoss 0.0186\tPrec@1 99.371\n",
      "Epoch: [7][1600/3750]\tLoss 0.0193\tPrec@1 99.354\n",
      "Epoch: [7][1700/3750]\tLoss 0.0198\tPrec@1 99.331\n",
      "Epoch: [7][1800/3750]\tLoss 0.0199\tPrec@1 99.327\n",
      "Epoch: [7][1900/3750]\tLoss 0.0199\tPrec@1 99.326\n",
      "Epoch: [7][2000/3750]\tLoss 0.0199\tPrec@1 99.319\n"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "learner_ag_news.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b755b6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Memory compute"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
