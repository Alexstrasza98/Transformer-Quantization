{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "071a4240",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5d29b0aa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from utils.data_utils import AG_NEWS_DATASET\n",
    "from utils.constants import *\n",
    "from utils.training import Learner\n",
    "\n",
    "from quantization.binarize import binarize, IRLinear\n",
    "from quantization.transformer import Transformer\n",
    "from quantization.quantize import quantizer\n",
    "from quantization.pytorch_api import ModelQuant\n",
    "from quantization.fully_quantize import Model as fullyQuantModel\n",
    "\n",
    "from utils.train_utils import change_t\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "49ac3637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, quant_type=None, quant_method=None, bit_num=None, quant_pattern=None):\n",
    "    '''\n",
    "    Create training model based on sepcified quant_type\n",
    "    ----------\n",
    "    Arguments:\n",
    "    quant_type    - quant type, should be one of [None, 'quantization', 'binarization']\n",
    "    quant_method  - quant method to use, if quant_type is None, it should also be None\n",
    "                    For 'quantization', should be one of ['basic', 'pytorch', 'fully']\n",
    "                    For 'binarization', should be one of ['basic', 'ir']\n",
    "    bit_num       - bit number for each parameter, only works when quant_type is 'quantization'\n",
    "                    should be one of [8,4,2]\n",
    "    quant_pattern - quantization pattern, should be one of ['MHA', 'FFN', 'CLS', 'ALL']\n",
    "    '''\n",
    "    model = Transformer(d_model=BASELINE_MODEL_DIM,\n",
    "                             d_ff=BASELINE_FFN_DIM,\n",
    "                             d_hidden=BASELINE_HIDDEN_DIM,\n",
    "                             h=BASELINE_MODEL_NUMBER_OF_HEADS,\n",
    "                             n_layers=BASELINE_MODEL_NUMBER_OF_LAYERS,\n",
    "                             n_class=4,\n",
    "                             vocab=vocab_size\n",
    "                            )\n",
    "    \n",
    "    __quant_type__ = [None,'quantization','binarization']\n",
    "    __bit_num__ = [None,8,4,2]\n",
    "    __quant_pattern__ = [None,'MHA', 'FFN', 'CLS', 'ALL']\n",
    "    \n",
    "    assert quant_type in __quant_type__, f\"Unimplemented quantization type, should be one of {__quant_type__}, got '{quant_type}'!\"\n",
    "    assert bit_num in __bit_num__, f\"Unimplemented bit number, should be one of {__bit_num__}, got '{bit_num}'!\"\n",
    "    assert quant_pattern in __quant_pattern__, f\"Unimplemented quantization method, should be one of {__quant_pattern__}, got '{quant_pattern}'!\"\n",
    "    \n",
    "    if quant_type == None:\n",
    "        if quant_method is not None:\n",
    "            print(f\"Quant method {quant_method} will not work in baseline model!\")\n",
    "        if bit_num is not None:\n",
    "            print(f\"Bit number {bit_num} will not work in baseline model!\")\n",
    "        if quant_pattern is not None:\n",
    "            print(f\"Quant pattern {quant_pattern} will not work in baseline model!\")\n",
    "    \n",
    "    elif quant_type == 'quantization':\n",
    "        __quant_method__ = ['basic', 'pytorch', 'fully']\n",
    "        \n",
    "        assert quant_method in __quant_method__, f\"Unimplemented quantization method, should be one of {__quant_method__}, got '{quant_method}'!\"\n",
    "        assert bit_num != None, f\"Bit number can not be None!\"\n",
    "        assert quant_pattern != None, f\"Quant pattern can not be None!\"\n",
    "        \n",
    "        if quant_method == 'basic':\n",
    "            if quant_pattern != 'ALL':\n",
    "                print(f\"Current quant method {quant_method} can only quantize the whole network, quant pattern {quant_pattern} will not work!\")\n",
    "            model = quantizer(model, bit_num, True)\n",
    "            \n",
    "        elif quant_method == 'pytorch':\n",
    "            model = ModelQuant(d_model=BASELINE_MODEL_DIM,\n",
    "                               d_ff=BASELINE_FFN_DIM,\n",
    "                               d_hidden=BASELINE_HIDDEN_DIM,\n",
    "                               h=BASELINE_MODEL_NUMBER_OF_HEADS,\n",
    "                               n_layers=BASELINE_MODEL_NUMBER_OF_LAYERS,\n",
    "                               n_class=4,\n",
    "                               vocab=tokenizer.vocab_size,\n",
    "                               quant_ffn=((quant_pattern == 'FFN')|(quant_pattern == 'ALL')),\n",
    "                               quant_mha=((quant_pattern == 'MHA')|(quant_pattern == 'ALL')),\n",
    "                               quant_classifier=((quant_pattern == 'CLS')|(quant_pattern == 'ALL')),\n",
    "                               bit_num=bit_num)\n",
    "            \n",
    "        elif quant_method == 'fully':\n",
    "            print(\"For fully_quantized model, bit number and quant pattern will not work!\")\n",
    "            model = fullyQuantModel(4,\n",
    "                tokenizer.vocab_size,\n",
    "                BASELINE_MODEL_NUMBER_OF_LAYERS,\n",
    "                BASELINE_MODEL_NUMBER_OF_HEADS,\n",
    "                BASELINE_MODEL_DIM)\n",
    "            \n",
    "    elif quant_type == 'binarization':\n",
    "        __quant_method__ = ['basic', 'ir']\n",
    "        assert quant_method in __quant_method__, f\"Unimplemented quantization method, should be one of {__quant_method__}, got '{quant_method}'!\"\n",
    "        assert quant_pattern != None, f\"Quant pattern can not be None!\"\n",
    "        print(f\"For binarization model, bit num will not work!\")\n",
    "        \n",
    "        binarize(model, quant_pattern, skip_final=True, qk_only=True)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3f25f80",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (input_embeddings): Embeddings(\n",
      "    (token_embedding): Embedding(30522, 512)\n",
      "    (pos_embedding): Embedding(512, 512)\n",
      "  )\n",
      "  (input_encodings): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (sublayer_attention): ModuleList(\n",
      "    (0): sublayerConnectionAttention(\n",
      "      (multiheads): MultiheadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): IRLinear (512 -> 512)\n",
      "          (1): IRLinear (512 -> 512)\n",
      "          (2): IRLinear (512 -> 512)\n",
      "        )\n",
      "        (output): IRLinear (512 -> 512)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layernorm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): sublayerConnectionAttention(\n",
      "      (multiheads): MultiheadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): IRLinear (512 -> 512)\n",
      "          (1): IRLinear (512 -> 512)\n",
      "          (2): IRLinear (512 -> 512)\n",
      "        )\n",
      "        (output): IRLinear (512 -> 512)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layernorm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (sublayer_ffn): ModuleList(\n",
      "    (0): sublayerConnectionFFN(\n",
      "      (ffn): PositionalWiseFFN(\n",
      "        (w_1): IRLinear (512 -> 1024)\n",
      "        (w_2): IRLinear (1024 -> 512)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layernorm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): sublayerConnectionFFN(\n",
      "      (ffn): PositionalWiseFFN(\n",
      "        (w_1): IRLinear (512 -> 1024)\n",
      "        (w_2): IRLinear (1024 -> 512)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layernorm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Classifier(\n",
      "    (hidden): IRLinear (512 -> 1024)\n",
      "    (classifier): Linear(in_features=1024, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_dl, test_dl = AG_NEWS_DATASET(tokenizer, batch_size = BATCH_SIZE).load_data()\n",
    "\n",
    "# create model\n",
    "model = Transformer(d_model=BASELINE_MODEL_DIM,\n",
    "                    d_ff=BASELINE_FFN_DIM,\n",
    "                    d_hidden=BASELINE_HIDDEN_DIM,\n",
    "                    h=BASELINE_MODEL_NUMBER_OF_HEADS,\n",
    "                    n_layers=BASELINE_MODEL_NUMBER_OF_LAYERS,\n",
    "                    n_class=4,\n",
    "                    vocab=tokenizer.vocab_size\n",
    "                   )\n",
    "\n",
    "binarize(model, 'ALL', binarize_layer='ir', skip_final=True, qk_only=False)\n",
    "print(model)\n",
    "\n",
    "# loss func\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# baseline training config -> do not change!\n",
    "optim = Adam(model.parameters(), lr= 1e-4)\n",
    "scheduler = MultiStepLR(optim, milestones=[10,15], gamma=0.1)\n",
    "\n",
    "train_config ={'model': model,\n",
    "               'loss_fn': loss_fn,\n",
    "               'optim': optim,\n",
    "               'scheduler': scheduler,\n",
    "               'datasets': [train_dl, test_dl],\n",
    "               'epochs': 10,\n",
    "               'batch_size': BATCH_SIZE\n",
    "               }\n",
    "\n",
    "train_config['exp_name'] = 'transformer_IRNET'\n",
    "\n",
    "# training\n",
    "learner_ag_news = Learner(train_config, ir = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a9d342d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.sublayer_attention[0].multiheads.heads[1].t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7321ebf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current lr 1.00000e-04\n",
      "tensor([0.1000], device='cuda:0')\n",
      "Epoch: [0][0/3750]\tLoss 21165.4707\tPrec@1 31.250\n",
      "Epoch: [0][100/3750]\tLoss 24068.9844\tPrec@1 31.374\n",
      "Epoch: [0][200/3750]\tLoss 17295.1633\tPrec@1 29.058\n",
      "Epoch: [0][300/3750]\tLoss 14339.7965\tPrec@1 28.686\n",
      "Epoch: [0][400/3750]\tLoss 12647.1906\tPrec@1 28.265\n",
      "Epoch: [0][500/3750]\tLoss 11378.3273\tPrec@1 28.412\n",
      "Epoch: [0][600/3750]\tLoss 10572.3136\tPrec@1 28.026\n",
      "Epoch: [0][700/3750]\tLoss 9866.4458\tPrec@1 27.960\n",
      "Epoch: [0][800/3750]\tLoss 9347.8048\tPrec@1 27.809\n",
      "Epoch: [0][900/3750]\tLoss 8874.2556\tPrec@1 27.629\n",
      "Epoch: [0][1000/3750]\tLoss 8416.5995\tPrec@1 27.691\n",
      "Epoch: [0][1100/3750]\tLoss 8019.8246\tPrec@1 27.725\n",
      "Epoch: [0][1200/3750]\tLoss 7681.0817\tPrec@1 27.597\n",
      "Epoch: [0][1300/3750]\tLoss 7355.8397\tPrec@1 27.693\n",
      "Epoch: [0][1400/3750]\tLoss 7066.3091\tPrec@1 27.663\n",
      "Epoch: [0][1500/3750]\tLoss 6801.2076\tPrec@1 27.625\n",
      "Epoch: [0][1600/3750]\tLoss 6558.7160\tPrec@1 27.567\n",
      "Epoch: [0][1700/3750]\tLoss 6338.2557\tPrec@1 27.504\n",
      "Epoch: [0][1800/3750]\tLoss 6112.6509\tPrec@1 27.672\n",
      "Epoch: [0][1900/3750]\tLoss 5919.8441\tPrec@1 27.676\n",
      "Epoch: [0][2000/3750]\tLoss 5741.1802\tPrec@1 27.689\n",
      "Epoch: [0][2100/3750]\tLoss 5567.3728\tPrec@1 27.618\n",
      "Epoch: [0][2200/3750]\tLoss 5403.6561\tPrec@1 27.608\n",
      "Epoch: [0][2300/3750]\tLoss 5246.8316\tPrec@1 27.639\n",
      "Epoch: [0][2400/3750]\tLoss 5098.5375\tPrec@1 27.582\n",
      "Epoch: [0][2500/3750]\tLoss 4956.4743\tPrec@1 27.626\n",
      "Epoch: [0][2600/3750]\tLoss 4827.5463\tPrec@1 27.755\n",
      "Epoch: [0][2700/3750]\tLoss 4695.6037\tPrec@1 27.781\n",
      "Epoch: [0][2800/3750]\tLoss 4573.5894\tPrec@1 27.730\n",
      "Epoch: [0][2900/3750]\tLoss 4450.4638\tPrec@1 27.791\n",
      "Epoch: [0][3000/3750]\tLoss 4343.4380\tPrec@1 27.746\n",
      "Epoch: [0][3100/3750]\tLoss 4243.3241\tPrec@1 27.809\n",
      "Epoch: [0][3200/3750]\tLoss 4150.2303\tPrec@1 27.806\n",
      "Epoch: [0][3300/3750]\tLoss 4054.2981\tPrec@1 27.863\n",
      "Epoch: [0][3400/3750]\tLoss 3967.9338\tPrec@1 27.868\n",
      "Epoch: [0][3500/3750]\tLoss 3881.1847\tPrec@1 27.903\n",
      "Epoch: [0][3600/3750]\tLoss 3798.9225\tPrec@1 27.874\n",
      "Epoch: [0][3700/3750]\tLoss 3720.2124\tPrec@1 27.886\n",
      "Epoch[0] *Validation*: Prec@1 30.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▎                                      | 1/10 [01:49<16:22, 109.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current lr 1.00000e-04\n",
      "tensor([0.2887], device='cuda:0')\n",
      "Epoch: [1][0/3750]\tLoss 585.1575\tPrec@1 15.625\n",
      "Epoch: [1][100/3750]\tLoss 2129.6004\tPrec@1 32.147\n",
      "Epoch: [1][200/3750]\tLoss 1411.7155\tPrec@1 30.193\n",
      "Epoch: [1][300/3750]\tLoss 1134.2423\tPrec@1 29.527\n",
      "Epoch: [1][400/3750]\tLoss 982.5048\tPrec@1 28.858\n",
      "Epoch: [1][500/3750]\tLoss 908.4314\tPrec@1 29.148\n",
      "Epoch: [1][600/3750]\tLoss 883.7329\tPrec@1 29.201\n",
      "Epoch: [1][700/3750]\tLoss 833.7551\tPrec@1 29.017\n",
      "Epoch: [1][800/3750]\tLoss 807.0178\tPrec@1 28.792\n",
      "Epoch: [1][900/3750]\tLoss 778.5893\tPrec@1 28.548\n",
      "Epoch: [1][1000/3750]\tLoss 745.7690\tPrec@1 28.553\n",
      "Epoch: [1][1100/3750]\tLoss 718.5661\tPrec@1 28.590\n",
      "Epoch: [1][1200/3750]\tLoss 698.2553\tPrec@1 28.450\n",
      "Epoch: [1][1300/3750]\tLoss 685.3663\tPrec@1 28.550\n",
      "Epoch: [1][1400/3750]\tLoss 671.2385\tPrec@1 28.437\n",
      "Epoch: [1][1500/3750]\tLoss 652.4644\tPrec@1 28.423\n",
      "Epoch: [1][1600/3750]\tLoss 637.8115\tPrec@1 28.508\n",
      "Epoch: [1][1700/3750]\tLoss 627.9484\tPrec@1 28.516\n",
      "Epoch: [1][1800/3750]\tLoss 617.0398\tPrec@1 28.585\n",
      "Epoch: [1][1900/3750]\tLoss 614.6430\tPrec@1 28.566\n",
      "Epoch: [1][2000/3750]\tLoss 606.1164\tPrec@1 28.558\n",
      "Epoch: [1][2100/3750]\tLoss 600.6387\tPrec@1 28.458\n",
      "Epoch: [1][2200/3750]\tLoss 588.5853\tPrec@1 28.605\n",
      "Epoch: [1][2300/3750]\tLoss 578.3305\tPrec@1 28.655\n",
      "Epoch: [1][2400/3750]\tLoss 567.9643\tPrec@1 28.689\n",
      "Epoch: [1][2500/3750]\tLoss 560.5709\tPrec@1 28.717\n",
      "Epoch: [1][2600/3750]\tLoss 556.4536\tPrec@1 28.855\n",
      "Epoch: [1][2700/3750]\tLoss 549.8273\tPrec@1 28.877\n",
      "Epoch: [1][2800/3750]\tLoss 541.8319\tPrec@1 28.874\n",
      "Epoch: [1][2900/3750]\tLoss 534.9798\tPrec@1 28.952\n",
      "Epoch: [1][3000/3750]\tLoss 530.0929\tPrec@1 28.954\n",
      "Epoch: [1][3100/3750]\tLoss 526.3261\tPrec@1 29.013\n",
      "Epoch: [1][3200/3750]\tLoss 530.7533\tPrec@1 28.979\n",
      "Epoch: [1][3300/3750]\tLoss 527.2385\tPrec@1 29.032\n",
      "Epoch: [1][3400/3750]\tLoss 525.1544\tPrec@1 29.015\n",
      "Epoch: [1][3500/3750]\tLoss 519.9534\tPrec@1 29.034\n",
      "Epoch: [1][3600/3750]\tLoss 514.9832\tPrec@1 29.065\n",
      "Epoch: [1][3700/3750]\tLoss 511.6324\tPrec@1 29.082\n",
      "Epoch[1] *Validation*: Prec@1 34.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████▌                                  | 2/10 [03:36<14:25, 108.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current lr 1.00000e-04\n",
      "tensor([0.8337], device='cuda:0')\n",
      "Epoch: [2][0/3750]\tLoss 381.4594\tPrec@1 15.625\n",
      "Epoch: [2][100/3750]\tLoss 483.5287\tPrec@1 33.045\n",
      "Epoch: [2][200/3750]\tLoss 344.0880\tPrec@1 30.737\n",
      "Epoch: [2][300/3750]\tLoss 279.3286\tPrec@1 30.866\n",
      "Epoch: [2][400/3750]\tLoss 240.7345\tPrec@1 30.884\n",
      "Epoch: [2][500/3750]\tLoss 215.5271\tPrec@1 31.524\n",
      "Epoch: [2][600/3750]\tLoss 203.1654\tPrec@1 31.474\n",
      "Epoch: [2][700/3750]\tLoss 188.9650\tPrec@1 31.624\n",
      "Epoch: [2][800/3750]\tLoss 177.8251\tPrec@1 31.726\n",
      "Epoch: [2][900/3750]\tLoss 168.7909\tPrec@1 31.642\n",
      "Epoch: [2][1000/3750]\tLoss 160.4991\tPrec@1 31.721\n",
      "Epoch: [2][1100/3750]\tLoss 154.9529\tPrec@1 31.747\n",
      "Epoch: [2][1200/3750]\tLoss 149.0420\tPrec@1 31.841\n",
      "Epoch: [2][1300/3750]\tLoss 143.4316\tPrec@1 32.208\n",
      "Epoch: [2][1400/3750]\tLoss 139.5627\tPrec@1 32.084\n",
      "Epoch: [2][1500/3750]\tLoss 135.3463\tPrec@1 32.174\n",
      "Epoch: [2][1600/3750]\tLoss 132.0684\tPrec@1 32.224\n",
      "Epoch: [2][1700/3750]\tLoss 129.3862\tPrec@1 32.251\n",
      "Epoch: [2][1800/3750]\tLoss 127.1842\tPrec@1 32.425\n",
      "Epoch: [2][1900/3750]\tLoss 126.0929\tPrec@1 32.471\n",
      "Epoch: [2][2000/3750]\tLoss 124.1829\tPrec@1 32.617\n",
      "Epoch: [2][2100/3750]\tLoss 122.8323\tPrec@1 32.650\n",
      "Epoch: [2][2200/3750]\tLoss 121.4324\tPrec@1 32.803\n",
      "Epoch: [2][2300/3750]\tLoss 119.1866\tPrec@1 33.062\n",
      "Epoch: [2][2400/3750]\tLoss 118.0749\tPrec@1 33.205\n",
      "Epoch: [2][2500/3750]\tLoss 116.2722\tPrec@1 33.443\n",
      "Epoch: [2][2600/3750]\tLoss 115.3614\tPrec@1 33.594\n",
      "Epoch: [2][2700/3750]\tLoss 114.4744\tPrec@1 33.684\n",
      "Epoch: [2][2800/3750]\tLoss 113.3469\tPrec@1 33.843\n",
      "Epoch: [2][2900/3750]\tLoss 112.0441\tPrec@1 34.110\n",
      "Epoch: [2][3000/3750]\tLoss 111.3960\tPrec@1 34.232\n",
      "Epoch: [2][3100/3750]\tLoss 111.5043\tPrec@1 34.316\n",
      "Epoch: [2][3200/3750]\tLoss 111.2005\tPrec@1 34.458\n",
      "Epoch: [2][3300/3750]\tLoss 110.4664\tPrec@1 34.643\n",
      "Epoch: [2][3400/3750]\tLoss 110.2730\tPrec@1 34.720\n",
      "Epoch: [2][3500/3750]\tLoss 110.0697\tPrec@1 34.855\n",
      "Epoch: [2][3600/3750]\tLoss 110.3859\tPrec@1 35.061\n",
      "Epoch: [2][3700/3750]\tLoss 109.9379\tPrec@1 35.273\n",
      "Epoch[2] *Validation*: Prec@1 54.250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████▉                              | 3/10 [05:22<12:29, 107.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current lr 1.00000e-04\n",
      "tensor([2.4074], device='cuda:0')\n",
      "Epoch: [3][0/3750]\tLoss 1526.2471\tPrec@1 0.000\n",
      "Epoch: [3][100/3750]\tLoss 2189.3204\tPrec@1 36.231\n",
      "Epoch: [3][200/3750]\tLoss 1474.9435\tPrec@1 36.427\n",
      "Epoch: [3][300/3750]\tLoss 1146.7469\tPrec@1 37.407\n",
      "Epoch: [3][400/3750]\tLoss 947.3324\tPrec@1 38.700\n",
      "Epoch: [3][500/3750]\tLoss 813.3875\tPrec@1 40.675\n",
      "Epoch: [3][600/3750]\tLoss 724.4939\tPrec@1 41.535\n",
      "Epoch: [3][700/3750]\tLoss 655.9231\tPrec@1 42.497\n",
      "Epoch: [3][800/3750]\tLoss 599.4893\tPrec@1 43.473\n",
      "Epoch: [3][900/3750]\tLoss 553.6310\tPrec@1 44.374\n",
      "Epoch: [3][1000/3750]\tLoss 514.6924\tPrec@1 45.174\n",
      "Epoch: [3][1100/3750]\tLoss 482.2177\tPrec@1 45.967\n",
      "Epoch: [3][1200/3750]\tLoss 454.8097\tPrec@1 46.524\n",
      "Epoch: [3][1300/3750]\tLoss 430.4817\tPrec@1 47.451\n",
      "Epoch: [3][1400/3750]\tLoss 408.8771\tPrec@1 48.022\n",
      "Epoch: [3][1500/3750]\tLoss 389.3861\tPrec@1 48.709\n",
      "Epoch: [3][1600/3750]\tLoss 373.7526\tPrec@1 48.969\n",
      "Epoch: [3][1700/3750]\tLoss 360.2273\tPrec@1 49.405\n",
      "Epoch: [3][1800/3750]\tLoss 346.6165\tPrec@1 50.134\n",
      "Epoch: [3][1900/3750]\tLoss 335.0890\tPrec@1 50.526\n",
      "Epoch: [3][2000/3750]\tLoss 323.6279\tPrec@1 51.153\n",
      "Epoch: [3][2100/3750]\tLoss 314.3009\tPrec@1 51.620\n",
      "Epoch: [3][2200/3750]\tLoss 305.4219\tPrec@1 52.137\n",
      "Epoch: [3][2300/3750]\tLoss 297.8144\tPrec@1 52.632\n",
      "Epoch: [3][2400/3750]\tLoss 289.8087\tPrec@1 53.148\n",
      "Epoch: [3][2500/3750]\tLoss 281.9510\tPrec@1 53.782\n",
      "Epoch: [3][2600/3750]\tLoss 275.9155\tPrec@1 54.274\n",
      "Epoch: [3][2700/3750]\tLoss 270.8666\tPrec@1 54.725\n",
      "Epoch: [3][2800/3750]\tLoss 265.4170\tPrec@1 55.145\n",
      "Epoch: [3][2900/3750]\tLoss 259.9683\tPrec@1 55.604\n",
      "Epoch: [3][3000/3750]\tLoss 257.0375\tPrec@1 55.911\n",
      "Epoch: [3][3100/3750]\tLoss 254.3727\tPrec@1 56.183\n",
      "Epoch: [3][3200/3750]\tLoss 251.8288\tPrec@1 56.411\n",
      "Epoch: [3][3300/3750]\tLoss 251.9698\tPrec@1 56.698\n",
      "Epoch: [3][3400/3750]\tLoss 253.0385\tPrec@1 56.897\n",
      "Epoch: [3][3500/3750]\tLoss 250.7398\tPrec@1 57.228\n",
      "Epoch: [3][3600/3750]\tLoss 247.1901\tPrec@1 57.645\n",
      "Epoch: [3][3700/3750]\tLoss 243.6156\tPrec@1 58.028\n",
      "Epoch[3] *Validation*: Prec@1 78.461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████▏                         | 4/10 [07:07<10:36, 106.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current lr 1.00000e-04\n",
      "tensor([6.9513], device='cuda:0')\n",
      "Epoch: [4][0/3750]\tLoss 1353.9819\tPrec@1 6.250\n",
      "Epoch: [4][100/3750]\tLoss 2900.7069\tPrec@1 51.918\n",
      "Epoch: [4][200/3750]\tLoss 1796.2151\tPrec@1 59.484\n",
      "Epoch: [4][300/3750]\tLoss 1353.6102\tPrec@1 62.583\n",
      "Epoch: [4][400/3750]\tLoss 1104.4719\tPrec@1 64.885\n",
      "Epoch: [4][500/3750]\tLoss 941.6392\tPrec@1 66.791\n",
      "Epoch: [4][600/3750]\tLoss 832.6884\tPrec@1 67.752\n",
      "Epoch: [4][700/3750]\tLoss 746.2222\tPrec@1 68.848\n",
      "Epoch: [4][800/3750]\tLoss 680.6020\tPrec@1 69.573\n",
      "Epoch: [4][900/3750]\tLoss 627.2893\tPrec@1 70.255\n",
      "Epoch: [4][1000/3750]\tLoss 583.3240\tPrec@1 70.892\n",
      "Epoch: [4][1100/3750]\tLoss 545.4357\tPrec@1 71.463\n",
      "Epoch: [4][1200/3750]\tLoss 513.0402\tPrec@1 71.865\n",
      "Epoch: [4][1300/3750]\tLoss 484.8997\tPrec@1 72.360\n",
      "Epoch: [4][1400/3750]\tLoss 461.9597\tPrec@1 72.667\n",
      "Epoch: [4][1500/3750]\tLoss 441.2969\tPrec@1 72.843\n",
      "Epoch: [4][1600/3750]\tLoss 422.6132\tPrec@1 73.064\n",
      "Epoch: [4][1700/3750]\tLoss 407.5584\tPrec@1 73.212\n",
      "Epoch: [4][1800/3750]\tLoss 394.4210\tPrec@1 73.510\n",
      "Epoch: [4][1900/3750]\tLoss 383.7573\tPrec@1 73.516\n",
      "Epoch: [4][2000/3750]\tLoss 370.8922\tPrec@1 73.821\n",
      "Epoch: [4][2100/3750]\tLoss 360.2383\tPrec@1 73.998\n",
      "Epoch: [4][2200/3750]\tLoss 349.9276\tPrec@1 74.243\n",
      "Epoch: [4][2300/3750]\tLoss 341.1534\tPrec@1 74.474\n",
      "Epoch: [4][2400/3750]\tLoss 331.6198\tPrec@1 74.753\n",
      "Epoch: [4][2500/3750]\tLoss 322.8647\tPrec@1 75.075\n",
      "Epoch: [4][2600/3750]\tLoss 315.8323\tPrec@1 75.318\n",
      "Epoch: [4][2700/3750]\tLoss 308.2925\tPrec@1 75.583\n",
      "Epoch: [4][2800/3750]\tLoss 301.2161\tPrec@1 75.740\n",
      "Epoch: [4][2900/3750]\tLoss 293.8655\tPrec@1 75.998\n",
      "Epoch: [4][3000/3750]\tLoss 288.2085\tPrec@1 76.118\n",
      "Epoch: [4][3100/3750]\tLoss 282.3100\tPrec@1 76.286\n",
      "Epoch: [4][3200/3750]\tLoss 277.3829\tPrec@1 76.395\n",
      "Epoch: [4][3300/3750]\tLoss 272.2633\tPrec@1 76.562\n",
      "Epoch: [4][3400/3750]\tLoss 267.3938\tPrec@1 76.754\n",
      "Epoch: [4][3500/3750]\tLoss 263.1072\tPrec@1 76.881\n",
      "Epoch: [4][3600/3750]\tLoss 259.0807\tPrec@1 77.032\n",
      "Epoch: [4][3700/3750]\tLoss 254.9059\tPrec@1 77.200\n",
      "Epoch[4] *Validation*: Prec@1 84.447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████▌                     | 5/10 [08:52<08:49, 105.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current lr 1.00000e-04\n",
      "tensor([20.0717], device='cuda:0')\n",
      "Epoch: [5][0/3750]\tLoss 321.5333\tPrec@1 53.125\n",
      "Epoch: [5][100/3750]\tLoss 927.8808\tPrec@1 72.184\n",
      "Epoch: [5][200/3750]\tLoss 659.0438\tPrec@1 74.705\n",
      "Epoch: [5][300/3750]\tLoss 530.5450\tPrec@1 76.132\n",
      "Epoch: [5][400/3750]\tLoss 449.5120\tPrec@1 77.385\n",
      "Epoch: [5][500/3750]\tLoss 402.6307\tPrec@1 77.832\n",
      "Epoch: [5][600/3750]\tLoss 363.6493\tPrec@1 78.546\n",
      "Epoch: [5][700/3750]\tLoss 335.4043\tPrec@1 79.124\n",
      "Epoch: [5][800/3750]\tLoss 312.5256\tPrec@1 79.588\n",
      "Epoch: [5][900/3750]\tLoss 294.4162\tPrec@1 79.925\n",
      "Epoch: [5][1000/3750]\tLoss 278.3849\tPrec@1 80.317\n",
      "Epoch: [5][1100/3750]\tLoss 264.2070\tPrec@1 80.660\n",
      "Epoch: [5][1200/3750]\tLoss 252.5225\tPrec@1 80.821\n",
      "Epoch: [5][1300/3750]\tLoss 241.7470\tPrec@1 81.142\n",
      "Epoch: [5][1400/3750]\tLoss 233.4942\tPrec@1 81.312\n",
      "Epoch: [5][1500/3750]\tLoss 225.2546\tPrec@1 81.489\n",
      "Epoch: [5][1600/3750]\tLoss 217.8847\tPrec@1 81.625\n",
      "Epoch: [5][1700/3750]\tLoss 212.0884\tPrec@1 81.691\n",
      "Epoch: [5][1800/3750]\tLoss 207.5187\tPrec@1 81.823\n",
      "Epoch: [5][1900/3750]\tLoss 204.4886\tPrec@1 81.822\n",
      "Epoch: [5][2000/3750]\tLoss 198.9298\tPrec@1 81.948\n",
      "Epoch: [5][2100/3750]\tLoss 193.9409\tPrec@1 82.139\n",
      "Epoch: [5][2200/3750]\tLoss 189.6283\tPrec@1 82.235\n",
      "Epoch: [5][2300/3750]\tLoss 185.8935\tPrec@1 82.350\n",
      "Epoch: [5][2400/3750]\tLoss 181.9192\tPrec@1 82.490\n",
      "Epoch: [5][2500/3750]\tLoss 177.4321\tPrec@1 82.703\n",
      "Epoch: [5][2600/3750]\tLoss 174.3763\tPrec@1 82.803\n",
      "Epoch: [5][2700/3750]\tLoss 171.2894\tPrec@1 82.921\n",
      "Epoch: [5][2800/3750]\tLoss 167.9157\tPrec@1 83.033\n",
      "Epoch: [5][2900/3750]\tLoss 164.7956\tPrec@1 83.174\n",
      "Epoch: [5][3000/3750]\tLoss 162.1399\tPrec@1 83.250\n",
      "Epoch: [5][3100/3750]\tLoss 159.5899\tPrec@1 83.295\n",
      "Epoch: [5][3200/3750]\tLoss 157.2302\tPrec@1 83.334\n",
      "Epoch: [5][3300/3750]\tLoss 154.7537\tPrec@1 83.448\n",
      "Epoch: [5][3400/3750]\tLoss 152.8850\tPrec@1 83.509\n",
      "Epoch: [5][3500/3750]\tLoss 150.9741\tPrec@1 83.580\n",
      "Epoch: [5][3600/3750]\tLoss 149.0586\tPrec@1 83.648\n",
      "Epoch: [5][3700/3750]\tLoss 147.8802\tPrec@1 83.636\n",
      "Epoch[5] *Validation*: Prec@1 88.368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████▊                 | 6/10 [10:38<07:03, 105.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current lr 1.00000e-04\n",
      "tensor([57.9565], device='cuda:0')\n",
      "Epoch: [6][0/3750]\tLoss 70.6200\tPrec@1 81.250\n",
      "Epoch: [6][100/3750]\tLoss 305.5932\tPrec@1 78.218\n",
      "Epoch: [6][200/3750]\tLoss 201.5248\tPrec@1 82.136\n",
      "Epoch: [6][300/3750]\tLoss 165.3695\tPrec@1 83.731\n",
      "Epoch: [6][400/3750]\tLoss 142.9147\tPrec@1 84.882\n",
      "Epoch: [6][500/3750]\tLoss 130.6433\tPrec@1 85.492\n",
      "Epoch: [6][600/3750]\tLoss 122.2649\tPrec@1 85.727\n",
      "Epoch: [6][700/3750]\tLoss 116.5397\tPrec@1 85.966\n",
      "Epoch: [6][800/3750]\tLoss 109.6074\tPrec@1 86.326\n",
      "Epoch: [6][900/3750]\tLoss 105.5105\tPrec@1 86.380\n",
      "Epoch: [6][1000/3750]\tLoss 102.4837\tPrec@1 86.510\n",
      "Epoch: [6][1100/3750]\tLoss 99.9186\tPrec@1 86.572\n",
      "Epoch: [6][1200/3750]\tLoss 97.6165\tPrec@1 86.529\n",
      "Epoch: [6][1300/3750]\tLoss 95.3666\tPrec@1 86.652\n",
      "Epoch: [6][1400/3750]\tLoss 94.1276\tPrec@1 86.668\n",
      "Epoch: [6][1500/3750]\tLoss 92.6705\tPrec@1 86.651\n",
      "Epoch: [6][1600/3750]\tLoss 91.5177\tPrec@1 86.653\n",
      "Epoch: [6][1700/3750]\tLoss 90.5027\tPrec@1 86.651\n",
      "Epoch: [6][1800/3750]\tLoss 89.4899\tPrec@1 86.684\n",
      "Epoch: [6][1900/3750]\tLoss 88.5538\tPrec@1 86.678\n",
      "Epoch: [6][2000/3750]\tLoss 88.2415\tPrec@1 86.632\n",
      "Epoch: [6][2100/3750]\tLoss 88.0640\tPrec@1 86.581\n",
      "Epoch: [6][2200/3750]\tLoss 87.3395\tPrec@1 86.648\n",
      "Epoch: [6][2300/3750]\tLoss 86.7512\tPrec@1 86.704\n",
      "Epoch: [6][2400/3750]\tLoss 86.0419\tPrec@1 86.776\n",
      "Epoch: [6][2500/3750]\tLoss 84.8223\tPrec@1 86.889\n",
      "Epoch: [6][2600/3750]\tLoss 83.7766\tPrec@1 86.977\n",
      "Epoch: [6][2700/3750]\tLoss 82.6826\tPrec@1 87.092\n",
      "Epoch: [6][2800/3750]\tLoss 81.8327\tPrec@1 87.169\n",
      "Epoch: [6][2900/3750]\tLoss 80.9819\tPrec@1 87.251\n",
      "Epoch: [6][3000/3750]\tLoss 80.1755\tPrec@1 87.289\n",
      "Epoch: [6][3100/3750]\tLoss 79.8629\tPrec@1 87.302\n",
      "Epoch: [6][3200/3750]\tLoss 79.5910\tPrec@1 87.306\n",
      "Epoch: [6][3300/3750]\tLoss 79.1776\tPrec@1 87.345\n",
      "Epoch: [6][3400/3750]\tLoss 78.4652\tPrec@1 87.398\n",
      "Epoch: [6][3500/3750]\tLoss 78.1334\tPrec@1 87.408\n",
      "Epoch: [6][3600/3750]\tLoss 77.5230\tPrec@1 87.470\n",
      "Epoch: [6][3700/3750]\tLoss 77.2504\tPrec@1 87.497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████             | 7/10 [12:23<05:16, 105.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[6] *Validation*: Prec@1 87.789\n",
      "current lr 1.00000e-04\n",
      "tensor([167.3475], device='cuda:0')\n",
      "Epoch: [7][0/3750]\tLoss 12.1944\tPrec@1 90.625\n",
      "Epoch: [7][100/3750]\tLoss 255.7659\tPrec@1 80.012\n",
      "Epoch: [7][200/3750]\tLoss 163.2374\tPrec@1 84.375\n",
      "Epoch: [7][300/3750]\tLoss 132.1622\tPrec@1 86.140\n",
      "Epoch: [7][400/3750]\tLoss 115.2240\tPrec@1 86.884\n",
      "Epoch: [7][500/3750]\tLoss 104.2879\tPrec@1 87.531\n",
      "Epoch: [7][600/3750]\tLoss 96.5109\tPrec@1 87.890\n",
      "Epoch: [7][700/3750]\tLoss 89.6567\tPrec@1 88.249\n",
      "Epoch: [7][800/3750]\tLoss 84.1652\tPrec@1 88.534\n",
      "Epoch: [7][900/3750]\tLoss 81.9153\tPrec@1 88.495\n",
      "Epoch: [7][1000/3750]\tLoss 79.9148\tPrec@1 88.565\n",
      "Epoch: [7][1100/3750]\tLoss 78.1037\tPrec@1 88.624\n",
      "Epoch: [7][1200/3750]\tLoss 76.7946\tPrec@1 88.580\n",
      "Epoch: [7][1300/3750]\tLoss 74.9933\tPrec@1 88.747\n",
      "Epoch: [7][1400/3750]\tLoss 74.4707\tPrec@1 88.667\n",
      "Epoch: [7][1500/3750]\tLoss 73.7586\tPrec@1 88.635\n",
      "Epoch: [7][1600/3750]\tLoss 73.0431\tPrec@1 88.656\n",
      "Epoch: [7][1700/3750]\tLoss 72.3321\tPrec@1 88.700\n",
      "Epoch: [7][1800/3750]\tLoss 71.8247\tPrec@1 88.711\n",
      "Epoch: [7][1900/3750]\tLoss 71.8406\tPrec@1 88.693\n",
      "Epoch: [7][2000/3750]\tLoss 71.4625\tPrec@1 88.721\n",
      "Epoch: [7][2100/3750]\tLoss 71.3640\tPrec@1 88.712\n",
      "Epoch: [7][2200/3750]\tLoss 70.6274\tPrec@1 88.762\n",
      "Epoch: [7][2300/3750]\tLoss 70.0467\tPrec@1 88.797\n",
      "Epoch: [7][2400/3750]\tLoss 69.0570\tPrec@1 88.917\n",
      "Epoch: [7][2500/3750]\tLoss 67.7844\tPrec@1 89.067\n",
      "Epoch: [7][2600/3750]\tLoss 66.8461\tPrec@1 89.168\n",
      "Epoch: [7][2700/3750]\tLoss 66.2412\tPrec@1 89.235\n",
      "Epoch: [7][2800/3750]\tLoss 65.5564\tPrec@1 89.267\n",
      "Epoch: [7][2900/3750]\tLoss 64.7717\tPrec@1 89.357\n",
      "Epoch: [7][3000/3750]\tLoss 64.3879\tPrec@1 89.359\n",
      "Epoch: [7][3100/3750]\tLoss 63.8356\tPrec@1 89.401\n",
      "Epoch: [7][3200/3750]\tLoss 63.8252\tPrec@1 89.380\n",
      "Epoch: [7][3300/3750]\tLoss 63.4407\tPrec@1 89.449\n",
      "Epoch: [7][3400/3750]\tLoss 62.9648\tPrec@1 89.505\n",
      "Epoch: [7][3500/3750]\tLoss 62.6624\tPrec@1 89.533\n",
      "Epoch: [7][3600/3750]\tLoss 62.1860\tPrec@1 89.579\n",
      "Epoch: [7][3700/3750]\tLoss 62.3786\tPrec@1 89.550\n",
      "Epoch[7] *Validation*: Prec@1 88.421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████▍        | 8/10 [14:08<03:30, 105.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current lr 1.00000e-04\n",
      "tensor([483.2103], device='cuda:0')\n",
      "Epoch: [8][0/3750]\tLoss 42.7400\tPrec@1 84.375\n",
      "Epoch: [8][100/3750]\tLoss 114.7661\tPrec@1 85.705\n",
      "Epoch: [8][200/3750]\tLoss 86.0169\tPrec@1 87.889\n",
      "Epoch: [8][300/3750]\tLoss 75.0524\tPrec@1 88.777\n",
      "Epoch: [8][400/3750]\tLoss 68.2836\tPrec@1 89.238\n",
      "Epoch: [8][500/3750]\tLoss 66.4842\tPrec@1 89.583\n",
      "Epoch: [8][600/3750]\tLoss 64.1516\tPrec@1 89.715\n",
      "Epoch: [8][700/3750]\tLoss 62.0181\tPrec@1 89.921\n",
      "Epoch: [8][800/3750]\tLoss 59.6606\tPrec@1 90.137\n",
      "Epoch: [8][900/3750]\tLoss 58.9358\tPrec@1 90.174\n",
      "Epoch: [8][1000/3750]\tLoss 58.9339\tPrec@1 90.138\n",
      "Epoch: [8][1100/3750]\tLoss 58.0594\tPrec@1 90.222\n",
      "Epoch: [8][1200/3750]\tLoss 56.8734\tPrec@1 90.282\n",
      "Epoch: [8][1300/3750]\tLoss 55.5834\tPrec@1 90.416\n",
      "Epoch: [8][1400/3750]\tLoss 54.9903\tPrec@1 90.389\n",
      "Epoch: [8][1500/3750]\tLoss 54.6402\tPrec@1 90.390\n",
      "Epoch: [8][1600/3750]\tLoss 54.3203\tPrec@1 90.399\n",
      "Epoch: [8][1700/3750]\tLoss 54.0640\tPrec@1 90.362\n",
      "Epoch: [8][1800/3750]\tLoss 54.0598\tPrec@1 90.361\n",
      "Epoch: [8][1900/3750]\tLoss 53.7873\tPrec@1 90.344\n",
      "Epoch: [8][2000/3750]\tLoss 53.7101\tPrec@1 90.383\n",
      "Epoch: [8][2100/3750]\tLoss 53.6161\tPrec@1 90.396\n",
      "Epoch: [8][2200/3750]\tLoss 53.1964\tPrec@1 90.438\n",
      "Epoch: [8][2300/3750]\tLoss 52.5652\tPrec@1 90.504\n",
      "Epoch: [8][2400/3750]\tLoss 51.6762\tPrec@1 90.639\n",
      "Epoch: [8][2500/3750]\tLoss 50.7931\tPrec@1 90.771\n",
      "Epoch: [8][2600/3750]\tLoss 50.1940\tPrec@1 90.844\n",
      "Epoch: [8][2700/3750]\tLoss 49.7022\tPrec@1 90.862\n",
      "Epoch: [8][2800/3750]\tLoss 49.4143\tPrec@1 90.889\n",
      "Epoch: [8][2900/3750]\tLoss 48.9482\tPrec@1 90.933\n",
      "Epoch: [8][3000/3750]\tLoss 48.9043\tPrec@1 90.917\n",
      "Epoch: [8][3100/3750]\tLoss 48.7524\tPrec@1 90.935\n",
      "Epoch: [8][3200/3750]\tLoss 49.1717\tPrec@1 90.892\n",
      "Epoch: [8][3300/3750]\tLoss 49.0583\tPrec@1 90.932\n",
      "Epoch: [8][3400/3750]\tLoss 48.6565\tPrec@1 91.000\n",
      "Epoch: [8][3500/3750]\tLoss 48.3070\tPrec@1 91.043\n",
      "Epoch: [8][3600/3750]\tLoss 47.9474\tPrec@1 91.086\n",
      "Epoch: [8][3700/3750]\tLoss 47.7001\tPrec@1 91.103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████▋    | 9/10 [15:52<01:45, 105.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[8] *Validation*: Prec@1 87.895\n",
      "current lr 1.00000e-04\n",
      "tensor([1395.2537], device='cuda:0')\n",
      "Epoch: [9][0/3750]\tLoss 60.4715\tPrec@1 84.375\n",
      "Epoch: [9][100/3750]\tLoss 118.5935\tPrec@1 86.170\n",
      "Epoch: [9][200/3750]\tLoss 86.3018\tPrec@1 88.650\n",
      "Epoch: [9][300/3750]\tLoss 72.8353\tPrec@1 89.826\n",
      "Epoch: [9][400/3750]\tLoss 65.0825\tPrec@1 90.461\n",
      "Epoch: [9][500/3750]\tLoss 63.0385\tPrec@1 90.625\n",
      "Epoch: [9][600/3750]\tLoss 59.2699\tPrec@1 90.979\n",
      "Epoch: [9][700/3750]\tLoss 56.0551\tPrec@1 91.200\n",
      "Epoch: [9][800/3750]\tLoss 53.2268\tPrec@1 91.487\n",
      "Epoch: [9][900/3750]\tLoss 53.1662\tPrec@1 91.478\n",
      "Epoch: [9][1000/3750]\tLoss 52.0621\tPrec@1 91.574\n",
      "Epoch: [9][1100/3750]\tLoss 50.7428\tPrec@1 91.638\n",
      "Epoch: [9][1200/3750]\tLoss 50.8852\tPrec@1 91.517\n",
      "Epoch: [9][1300/3750]\tLoss 50.0650\tPrec@1 91.593\n",
      "Epoch: [9][1400/3750]\tLoss 49.8631\tPrec@1 91.537\n",
      "Epoch: [9][1500/3750]\tLoss 49.8908\tPrec@1 91.514\n",
      "Epoch: [9][1600/3750]\tLoss 49.6807\tPrec@1 91.511\n",
      "Epoch: [9][1700/3750]\tLoss 49.3541\tPrec@1 91.509\n",
      "Epoch: [9][1800/3750]\tLoss 48.9617\tPrec@1 91.555\n",
      "Epoch: [9][1900/3750]\tLoss 49.1174\tPrec@1 91.541\n",
      "Epoch: [9][2000/3750]\tLoss 49.4755\tPrec@1 91.495\n",
      "Epoch: [9][2100/3750]\tLoss 49.8030\tPrec@1 91.448\n",
      "Epoch: [9][2200/3750]\tLoss 49.6756\tPrec@1 91.473\n",
      "Epoch: [9][2300/3750]\tLoss 49.5173\tPrec@1 91.501\n",
      "Epoch: [9][2400/3750]\tLoss 48.8926\tPrec@1 91.587\n",
      "Epoch: [9][2500/3750]\tLoss 48.2478\tPrec@1 91.681\n",
      "Epoch: [9][2600/3750]\tLoss 47.7123\tPrec@1 91.750\n",
      "Epoch: [9][2700/3750]\tLoss 47.3881\tPrec@1 91.784\n",
      "Epoch: [9][2800/3750]\tLoss 47.1071\tPrec@1 91.831\n",
      "Epoch: [9][2900/3750]\tLoss 46.7234\tPrec@1 91.885\n",
      "Epoch: [9][3000/3750]\tLoss 46.7654\tPrec@1 91.874\n",
      "Epoch: [9][3100/3750]\tLoss 46.5317\tPrec@1 91.895\n",
      "Epoch: [9][3200/3750]\tLoss 46.4825\tPrec@1 91.896\n",
      "Epoch: [9][3300/3750]\tLoss 46.2227\tPrec@1 91.948\n",
      "Epoch: [9][3400/3750]\tLoss 46.1581\tPrec@1 91.967\n",
      "Epoch: [9][3500/3750]\tLoss 45.7826\tPrec@1 91.994\n",
      "Epoch: [9][3600/3750]\tLoss 45.4661\tPrec@1 92.032\n",
      "Epoch: [9][3700/3750]\tLoss 45.6873\tPrec@1 92.007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 10/10 [17:36<00:00, 105.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[9] *Validation*: Prec@1 87.855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[30.894736842105264,\n",
       " 34.89473684210526,\n",
       " 54.25,\n",
       " 78.46052631578948,\n",
       " 84.44736842105263,\n",
       " 88.36842105263158,\n",
       " 87.78947368421052,\n",
       " 88.42105263157895,\n",
       " 87.89473684210526,\n",
       " 87.85526315789474]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner_ag_news.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b755b6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Memory compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "07184be3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils.utils import count_memory_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "581cdf9e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82497552"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_memory_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "90a45c28",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_embeddings: 3.355749760294658\n",
      "input_encodings: 0.0\n",
      "sublayer_attention: 0.44420377699589014\n",
      "sublayer_ffn: 0.4439875142028055\n",
      "classifier: 0.11180870880130434\n"
     ]
    }
   ],
   "source": [
    "total = count_memory_size(model) - count_memory_size(model.input_embeddings)\n",
    "for name, layer in model.named_children():\n",
    "    print(f'{name}: {count_memory_size(layer)/total}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
