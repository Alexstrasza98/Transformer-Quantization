{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "071a4240",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d29b0aa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from utils.data_utils import AG_NEWS_DATASET\n",
    "from utils.constants import *\n",
    "from utils.training import Learner\n",
    "\n",
    "from quantization.binarize import binarize, IRLinear\n",
    "from quantization.transformer import Transformer\n",
    "from quantization.quantize import quantizer\n",
    "from quantization.pytorch_api import ModelQuant\n",
    "from quantization.fully_quantize import Model as fullyQuantModel\n",
    "\n",
    "from utils.train_utils import change_t\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ac3637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, quant_type=None, quant_method=None, bit_num=None, quant_pattern=None):\n",
    "    '''\n",
    "    Create training model based on sepcified quant_type\n",
    "    ----------\n",
    "    Arguments:\n",
    "    quant_type    - quant type, should be one of [None, 'quantization', 'binarization']\n",
    "    quant_method  - quant method to use, if quant_type is None, it should also be None\n",
    "                    For 'quantization', should be one of ['basic', 'pytorch', 'fully']\n",
    "                    For 'binarization', should be one of ['basic', 'ir']\n",
    "    bit_num       - bit number for each parameter, only works when quant_type is 'quantization'\n",
    "                    should be one of [8,4,2]\n",
    "    quant_pattern - quantization pattern, should be one of ['MHA', 'FFN', 'CLS', 'ALL']\n",
    "    '''\n",
    "    model = Transformer(d_model=BASELINE_MODEL_DIM,\n",
    "                             d_ff=BASELINE_FFN_DIM,\n",
    "                             d_hidden=BASELINE_HIDDEN_DIM,\n",
    "                             h=BASELINE_MODEL_NUMBER_OF_HEADS,\n",
    "                             n_layers=BASELINE_MODEL_NUMBER_OF_LAYERS,\n",
    "                             n_class=4,\n",
    "                             vocab=vocab_size\n",
    "                            )\n",
    "    \n",
    "    __quant_type__ = [None,'quantization','binarization']\n",
    "    __bit_num__ = [None,8,4,2]\n",
    "    __quant_pattern__ = [None,'MHA', 'FFN', 'CLS', 'ALL']\n",
    "    \n",
    "    assert quant_type in __quant_type__, f\"Unimplemented quantization type, should be one of {__quant_type__}, got '{quant_type}'!\"\n",
    "    assert bit_num in __bit_num__, f\"Unimplemented bit number, should be one of {__bit_num__}, got '{bit_num}'!\"\n",
    "    assert quant_pattern in __quant_pattern__, f\"Unimplemented quantization method, should be one of {__quant_pattern__}, got '{quant_pattern}'!\"\n",
    "    \n",
    "    if quant_type == None:\n",
    "        if quant_method is not None:\n",
    "            print(f\"Quant method {quant_method} will not work in baseline model!\")\n",
    "        if bit_num is not None:\n",
    "            print(f\"Bit number {bit_num} will not work in baseline model!\")\n",
    "        if quant_pattern is not None:\n",
    "            print(f\"Quant pattern {quant_pattern} will not work in baseline model!\")\n",
    "    \n",
    "    elif quant_type == 'quantization':\n",
    "        __quant_method__ = ['basic', 'pytorch', 'fully']\n",
    "        \n",
    "        assert quant_method in __quant_method__, f\"Unimplemented quantization method, should be one of {__quant_method__}, got '{quant_method}'!\"\n",
    "        assert bit_num != None, f\"Bit number can not be None!\"\n",
    "        assert quant_pattern != None, f\"Quant pattern can not be None!\"\n",
    "        \n",
    "        if quant_method == 'basic':\n",
    "            if quant_pattern != 'ALL':\n",
    "                print(f\"Current quant method {quant_method} can only quantize the whole network, quant pattern {quant_pattern} will not work!\")\n",
    "            model = quantizer(model, bit_num, True)\n",
    "            \n",
    "        elif quant_method == 'pytorch':\n",
    "            model = ModelQuant(d_model=BASELINE_MODEL_DIM,\n",
    "                               d_ff=BASELINE_FFN_DIM,\n",
    "                               d_hidden=BASELINE_HIDDEN_DIM,\n",
    "                               h=BASELINE_MODEL_NUMBER_OF_HEADS,\n",
    "                               n_layers=BASELINE_MODEL_NUMBER_OF_LAYERS,\n",
    "                               n_class=4,\n",
    "                               vocab=tokenizer.vocab_size,\n",
    "                               quant_ffn=((quant_pattern == 'FFN')|(quant_pattern == 'ALL')),\n",
    "                               quant_mha=((quant_pattern == 'MHA')|(quant_pattern == 'ALL')),\n",
    "                               quant_classifier=((quant_pattern == 'CLS')|(quant_pattern == 'ALL')),\n",
    "                               bit_num=bit_num)\n",
    "            \n",
    "        elif quant_method == 'fully':\n",
    "            print(\"For fully_quantized model, bit number and quant pattern will not work!\")\n",
    "            model = fullyQuantModel(4,\n",
    "                tokenizer.vocab_size,\n",
    "                BASELINE_MODEL_NUMBER_OF_LAYERS,\n",
    "                BASELINE_MODEL_NUMBER_OF_HEADS,\n",
    "                BASELINE_MODEL_DIM)\n",
    "            \n",
    "    elif quant_type == 'binarization':\n",
    "        __quant_method__ = ['basic', 'ir']\n",
    "        assert quant_method in __quant_method__, f\"Unimplemented quantization method, should be one of {__quant_method__}, got '{quant_method}'!\"\n",
    "        assert quant_pattern != None, f\"Quant pattern can not be None!\"\n",
    "        print(f\"For binarization model, bit num will not work!\")\n",
    "        \n",
    "        binarize(model, quant_pattern, skip_final=True, qk_only=True)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f25f80",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_dl, test_dl = AG_NEWS_DATASET(tokenizer, batch_size = BATCH_SIZE).load_data()\n",
    "\n",
    "# create model\n",
    "model = Transformer(d_model=BASELINE_MODEL_DIM,\n",
    "                    d_ff=BASELINE_FFN_DIM,\n",
    "                    d_hidden=BASELINE_HIDDEN_DIM,\n",
    "                    h=BASELINE_MODEL_NUMBER_OF_HEADS,\n",
    "                    n_layers=BASELINE_MODEL_NUMBER_OF_LAYERS,\n",
    "                    n_class=4,\n",
    "                    vocab=tokenizer.vocab_size\n",
    "                   )\n",
    "\n",
    "# binarize(model, 'ALL', binarize_layer='basic', skip_final=True, qk_only=False)\n",
    "# print(model)\n",
    "\n",
    "# loss func\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# baseline training config -> do not change!\n",
    "optim = Adam(model.parameters(), lr= 1e-4)\n",
    "scheduler = MultiStepLR(optim, milestones=[10,15], gamma=0.1)\n",
    "\n",
    "train_config ={'model': model,\n",
    "               'loss_fn': loss_fn,\n",
    "               'optim': optim,\n",
    "               'scheduler': scheduler,\n",
    "               'datasets': [train_dl, test_dl],\n",
    "               'epochs': 10,\n",
    "               'batch_size': BATCH_SIZE\n",
    "               }\n",
    "\n",
    "train_config['exp_name'] = 'transformer_baseline'\n",
    "\n",
    "# training\n",
    "learner_ag_news = Learner(train_config, ir = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7321ebf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "learner_ag_news.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b755b6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Memory compute"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
