{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2edbf7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from utils.data_utils import AG_NEWS_DATASET\n",
    "from utils.constants import *\n",
    "# from model.transformer import Transformer\n",
    "from training import Learner\n",
    "\n",
    "from quantization.binarize import binarize\n",
    "from quantization.transformer_raw import Transformer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3f25f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_dl, test_dl = AG_NEWS_DATASET(tokenizer, batch_size = BATCH_SIZE).load_data()\n",
    "\n",
    "# create model\n",
    "model = Transformer(d_model=BASELINE_MODEL_DIMENSION,\n",
    "                    vocab=tokenizer.vocab_size,\n",
    "                    h=BASELINE_MODEL_NUMBER_OF_HEADS,\n",
    "                    n_layers=BASELINE_MODEL_NUMBER_OF_LAYERS,\n",
    "                    n_class=4\n",
    "                   )\n",
    "model.model_name = 'transformer'\n",
    "\n",
    "# model_b = binarize(model, binarize_all_linear=True)\n",
    "# model_b.model_name = 'binary_transformer'\n",
    "\n",
    "# loss func\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# simple optimizer -> to improve\n",
    "optim = Adam(model.parameters(), lr= 1e-4)\n",
    "scheduler = MultiStepLR(optim, milestones=[10,15], gamma=0.1)\n",
    "\n",
    "train_config ={'model': model,\n",
    "               'loss_fn': loss_fn,\n",
    "               'optim': optim,\n",
    "               'scheduler': scheduler,\n",
    "               'datasets': [train_dl, test_dl],\n",
    "               'epochs': 10,\n",
    "               'batch_size': BATCH_SIZE\n",
    "               }\n",
    "\n",
    "# training\n",
    "learner_ag_news = Learner(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7321ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current lr 1.00000e-04\n",
      "Epoch: [0][0/3750]\tLoss 0.3273\tPrec@1 87.500\n",
      "Epoch: [0][100/3750]\tLoss 3.6267\tPrec@1 34.220\n",
      "Epoch: [0][200/3750]\tLoss 2.5518\tPrec@1 34.748\n",
      "Epoch: [0][300/3750]\tLoss 2.1056\tPrec@1 38.694\n",
      "Epoch: [0][400/3750]\tLoss 1.8062\tPrec@1 45.114\n",
      "Epoch: [0][500/3750]\tLoss 1.5801\tPrec@1 51.166\n",
      "Epoch: [0][600/3750]\tLoss 1.4141\tPrec@1 55.735\n",
      "Epoch: [0][700/3750]\tLoss 1.2767\tPrec@1 59.816\n",
      "Epoch: [0][800/3750]\tLoss 1.1698\tPrec@1 63.011\n",
      "Epoch: [0][900/3750]\tLoss 1.0910\tPrec@1 65.365\n",
      "Epoch: [0][1000/3750]\tLoss 1.0252\tPrec@1 67.386\n",
      "Epoch: [0][1100/3750]\tLoss 0.9680\tPrec@1 69.213\n",
      "Epoch: [0][1200/3750]\tLoss 0.9168\tPrec@1 70.785\n",
      "Epoch: [0][1300/3750]\tLoss 0.8741\tPrec@1 72.067\n",
      "Epoch: [0][1400/3750]\tLoss 0.8369\tPrec@1 73.238\n",
      "Epoch: [0][1500/3750]\tLoss 0.8058\tPrec@1 74.184\n",
      "Epoch: [0][1600/3750]\tLoss 0.7801\tPrec@1 74.977\n",
      "Epoch: [0][1700/3750]\tLoss 0.7550\tPrec@1 75.792\n",
      "Epoch: [0][1800/3750]\tLoss 0.7316\tPrec@1 76.496\n",
      "Epoch: [0][1900/3750]\tLoss 0.7148\tPrec@1 77.001\n",
      "Epoch: [0][2000/3750]\tLoss 0.6985\tPrec@1 77.488\n",
      "Epoch: [0][2100/3750]\tLoss 0.6808\tPrec@1 78.079\n",
      "Epoch: [0][2200/3750]\tLoss 0.6639\tPrec@1 78.609\n",
      "Epoch: [0][2300/3750]\tLoss 0.6490\tPrec@1 79.057\n",
      "Epoch: [0][2400/3750]\tLoss 0.6331\tPrec@1 79.583\n",
      "Epoch: [0][2500/3750]\tLoss 0.6160\tPrec@1 80.127\n",
      "Epoch: [0][2600/3750]\tLoss 0.6025\tPrec@1 80.545\n",
      "Epoch: [0][2700/3750]\tLoss 0.5911\tPrec@1 80.881\n",
      "Epoch: [0][2800/3750]\tLoss 0.5798\tPrec@1 81.206\n",
      "Epoch: [0][2900/3750]\tLoss 0.5692\tPrec@1 81.530\n",
      "Epoch: [0][3000/3750]\tLoss 0.5599\tPrec@1 81.807\n",
      "Epoch: [0][3100/3750]\tLoss 0.5515\tPrec@1 82.054\n",
      "Epoch: [0][3200/3750]\tLoss 0.5431\tPrec@1 82.314\n",
      "Epoch: [0][3300/3750]\tLoss 0.5350\tPrec@1 82.585\n",
      "Epoch: [0][3400/3750]\tLoss 0.5271\tPrec@1 82.835\n",
      "Epoch: [0][3500/3750]\tLoss 0.5195\tPrec@1 83.062\n",
      "Epoch: [0][3600/3750]\tLoss 0.5124\tPrec@1 83.285\n",
      "Epoch: [0][3700/3750]\tLoss 0.5060\tPrec@1 83.491\n",
      "Epoch[0] *Validation*: Prec@1 90.658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▎                                      | 1/10 [06:04<54:40, 364.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current lr 1.00000e-04\n",
      "Epoch: [1][0/3750]\tLoss 0.6588\tPrec@1 78.125\n",
      "Epoch: [1][100/3750]\tLoss 0.8462\tPrec@1 80.136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▎                                      | 1/10 [06:23<57:30, 383.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8431/3461410955.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner_ag_news\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Transformer-Quantization/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Transformer-Quantization/training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, epoch, verbose)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m# compute gradient and do SGD step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learner_ag_news.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
